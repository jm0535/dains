{
  "hash": "feb1259c1d2494b21edd0d1f57408525",
  "result": {
    "engine": "knitr",
    "markdown": "---\nprefer-html: true\n---\n\n# Advanced Modeling Techniques {#sec-advanced-modeling}\n\n::: {.callout-note}\n## Learning Objectives\n\nBy the end of this chapter, you will be able to:\n\n1. Interpret machine learning models using model-agnostic methods\n2. Calculate and visualize variable importance\n3. Create partial dependence and ICE plots\n4. Explain individual predictions with SHAP values\n5. Analyze time series data with temporal autocorrelation\n6. Decompose time series into trend, seasonal, and noise components\n7. Build ARIMA models for forecasting\n8. Account for temporal autocorrelation in regression models\n:::\n\n## Introduction\n\nThis chapter explores advanced modeling techniques that extend beyond traditional regression. We'll cover model interpretation methods that help us understand \"black box\" machine learning models, and time series analysis for data collected sequentially over time.\n\n## Model Interpretation and Explainability\n\nWhile machine learning models like Random Forest can achieve high predictive accuracy, understanding *why* they make specific predictions is crucial for ecological applications. Model interpretation helps us extract ecological insights and communicate results to stakeholders.\n\n### Variable Importance\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load packages for model interpretation\nlibrary(DALEX)\nlibrary(DALEXtra)\nlibrary(tidymodels)\nlibrary(palmerpenguins)\n\n# Prepare the penguins data (same as chapter 08)\npenguins <- palmerpenguins::penguins %>%\n  drop_na()\n\n# Split data into training and testing sets\nset.seed(123)\npenguin_split <- initial_split(penguins, prop = 0.75, strata = species)\npenguin_train <- training(penguin_split)\npenguin_test <- testing(penguin_split)\n\n# Define a recipe for preprocessing\npenguin_recipe <- recipe(body_mass_g ~ bill_length_mm + bill_depth_mm +\n                         flipper_length_mm + species,\n                         data = penguin_train) %>%\n  step_dummy(species) %>%\n  step_normalize(all_numeric_predictors()) %>%\n  step_zv(all_predictors())\n\n# Define Random Forest model specification\nrf_spec <- rand_forest(trees = 100) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"regression\")\n\n# Create workflow\nrf_wf <- workflow() %>%\n  add_recipe(penguin_recipe) %>%\n  add_model(rf_spec)\n\n# Fit the final model\nfinal_fit <- rf_wf %>%\n  fit(data = penguin_train)\n\n# Create an explainer object\nexplainer_rf <- explain_tidymodels(\n  final_fit,\n  data = penguin_test %>% select(-body_mass_g),\n  y = penguin_test$body_mass_g,\n  label = \"Random Forest\",\n  verbose = FALSE\n)\n\n# Calculate variable importance using permutation\nvi_rf <- model_parts(explainer_rf, loss_function = loss_root_mean_square)\n\n# Visualize variable importance\nplot(vi_rf) +\n  labs(title = \"Variable Importance for Body Mass Prediction\",\n       subtitle = \"Permutation-based importance shows contribution of each predictor\") +\n  theme_minimal()\n\n# Get the importance values\nvi_df <- as.data.frame(vi_rf)\nvi_summary <- vi_df %>%\n  filter(variable != \"_baseline_\", variable != \"_full_model_\") %>%\n  group_by(variable) %>%\n  summarize(\n    mean_dropout_loss = mean(dropout_loss),\n    .groups = \"drop\"\n  ) %>%\n  arrange(desc(mean_dropout_loss))\n\nprint(vi_summary)\n#> # A tibble: 7 × 2\n#>   variable          mean_dropout_loss\n#>   <chr>                         <dbl>\n#> 1 species                        574.\n#> 2 flipper_length_mm              536.\n#> 3 bill_depth_mm                  384.\n#> 4 bill_length_mm                 351.\n#> 5 island                         311.\n#> 6 sex                            311.\n#> 7 year                           311.\n```\n\n::: {.cell-output-display}\n![Variable importance for body mass prediction](09-advanced-modeling_files/figure-html/fig-variable-importance-1.png){#fig-variable-importance fig-align='center' width=100%}\n:::\n:::\n\n\n::: {.callout-note}\n## Code Explanation\n\nThis code demonstrates model-agnostic variable importance:\n\n1. **DALEX Framework**\n   - Creates an \"explainer\" object that wraps the tidymodels workflow\n   - Provides a unified interface for model interpretation\n   - Works with any model type (linear, tree-based, neural networks)\n\n2. **Permutation Importance**\n   - Measures how much model performance decreases when a variable is randomly shuffled\n   - Higher dropout loss = more important variable\n   - Unlike tree-based importance, this works for any model and accounts for correlations\n\n3. **Interpretation**\n   - Variables are ranked by their contribution to predictions\n   - Helps identify which morphological features are most predictive of body mass\n   - Provides ecological insights into species morphology\n:::\n\n### Partial Dependence Plots\n\nPartial dependence plots show how predictions change as a single variable varies, while averaging over all other variables.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Create partial dependence profiles for key variables\npdp_flipper <- model_profile(\n  explainer_rf,\n  variables = \"flipper_length_mm\",\n  N = NULL  # Use all observations\n)\n\npdp_bill <- model_profile(\n  explainer_rf,\n  variables = \"bill_length_mm\",\n  N = NULL\n)\n\n# Plot partial dependence\nplot(pdp_flipper) +\n  labs(title = \"Partial Dependence: Flipper Length\",\n       subtitle = \"Shows average effect of flipper length on predicted body mass\",\n       x = \"Flipper Length (mm)\",\n       y = \"Predicted Body Mass (g)\") +\n  theme_minimal()\n\nplot(pdp_bill) +\n  labs(title = \"Partial Dependence: Bill Length\",\n       subtitle = \"Shows average effect of bill length on predicted body mass\",\n       x = \"Bill Length (mm)\",\n       y = \"Predicted Body Mass (g)\") +\n  theme_minimal()\n\n# Create 2D partial dependence plot for interactions\npdp_2d <- model_profile(\n  explainer_rf,\n  variables = c(\"flipper_length_mm\", \"bill_length_mm\"),\n  N = 100\n)\n\n# Note: 2D plots require additional processing\n# For simplicity, we'll show individual effects\n```\n\n::: {.cell-output-display}\n![Partial dependence: Flipper length](09-advanced-modeling_files/figure-html/fig-partial-dependence-1.png){#fig-partial-dependence-1 fig-align='center' width=100%}\n:::\n\n::: {.cell-output-display}\n![Partial dependence: Bill length](09-advanced-modeling_files/figure-html/fig-partial-dependence-2.png){#fig-partial-dependence-2 fig-align='center' width=100%}\n:::\n:::\n\n\n::: {.callout-important}\n## Results Interpretation\n\nPartial dependence plots reveal important ecological relationships:\n\n1. **Marginal Effects**\n   - Shows how each predictor affects the response on average\n   - Accounts for correlations between predictors\n   - Reveals non-linear relationships that linear models would miss\n\n2. **Ecological Insights**\n   - Flipper length typically shows a strong positive relationship with body mass\n   - The relationship may be non-linear (e.g., diminishing returns at large sizes)\n   - Different species may show different patterns (captured by species variable)\n\n3. **Model Behavior**\n   - Helps validate that the model learned sensible relationships\n   - Can reveal unexpected patterns that warrant further investigation\n   - Useful for communicating model predictions to non-technical audiences\n:::\n\n### Individual Conditional Expectation (ICE) Plots\n\nWhile partial dependence shows average effects, ICE plots show how predictions change for individual observations.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Create ICE plots for flipper length\nice_flipper <- model_profile(\n  explainer_rf,\n  variables = \"flipper_length_mm\",\n  N = 50,  # Use 50 observations for clarity\n  type = \"conditional\"\n)\n\n# Plot ICE curves\nplot(ice_flipper) +\n  labs(title = \"Individual Conditional Expectation: Flipper Length\",\n       subtitle = \"Each line shows how prediction changes for one penguin\",\n       x = \"Flipper Length (mm)\",\n       y = \"Predicted Body Mass (g)\") +\n  theme_minimal()\n\n# Compare with partial dependence (average)\nplot(ice_flipper, geom = \"profiles\") +\n  labs(title = \"ICE Plots with Partial Dependence (yellow line)\",\n       subtitle = \"Individual effects vs. average effect\",\n       x = \"Flipper Length (mm)\",\n       y = \"Predicted Body Mass (g)\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![Individual conditional expectation curves](09-advanced-modeling_files/figure-html/fig-ice-plots-1.png){#fig-ice-plots-1 fig-align='center' width=100%}\n:::\n\n::: {.cell-output-display}\n![ICE plots with partial dependence overlay](09-advanced-modeling_files/figure-html/fig-ice-plots-2.png){#fig-ice-plots-2 fig-align='center' width=100%}\n:::\n:::\n\n\n::: {.callout-tip}\n## PROFESSIONAL TIP: Choosing Interpretation Methods\n\nWhen interpreting machine learning models for ecological applications:\n\n1. **Variable Importance**\n   - Use permutation importance for model-agnostic assessment\n   - Compare with domain knowledge to validate model behavior\n   - Consider both statistical and ecological importance\n   - Report confidence intervals when possible\n\n2. **Partial Dependence Plots**\n   - Show average marginal effects of predictors\n   - Useful for understanding overall patterns\n   - Can mask heterogeneous effects across observations\n   - Best for communicating general relationships\n\n3. **ICE Plots**\n   - Reveal individual-level heterogeneity\n   - Identify subgroups with different response patterns\n   - More complex to interpret than PDP\n   - Useful for detecting interactions\n\n4. **Break-Down Plots** (for individual predictions)\n   - Explain specific predictions step-by-step\n   - Useful for understanding outliers or unusual cases\n   - Helps build trust in model predictions\n   - Essential for high-stakes conservation decisions\n:::\n\n### Explaining Individual Predictions\n\nFor specific conservation decisions, we often need to understand why the model made a particular prediction.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Select an interesting observation to explain\nobservation_to_explain <- penguin_test[1, ]\n\ncat(\"Explaining prediction for:\\n\")\n#> Explaining prediction for:\nprint(observation_to_explain %>% select(species, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g))\n#> # A tibble: 1 × 5\n#>   species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n#>   <fct>            <dbl>         <dbl>             <int>       <int>\n#> 1 Adelie            39.1          18.7               181        3750\n\n# Create a break-down explanation\nbd <- predict_parts(\n  explainer_rf,\n  new_observation = observation_to_explain,\n  type = \"break_down\"\n)\n\n# Plot the break-down\nplot(bd) +\n  labs(title = \"Break-Down Plot for Individual Prediction\",\n       subtitle = \"Shows contribution of each variable to the final prediction\") +\n  theme_minimal()\n\n# Create SHAP values for more robust attribution\nshap <- predict_parts(\n  explainer_rf,\n  new_observation = observation_to_explain,\n  type = \"shap\",\n  B = 25  # Number of random orderings\n)\n\n# Plot SHAP values\nplot(shap) +\n  labs(title = \"SHAP Values for Individual Prediction\",\n       subtitle = \"Average contribution across all possible variable orderings\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![Break-down plot for individual prediction](09-advanced-modeling_files/figure-html/fig-individual-predictions-1.png){#fig-individual-predictions-1 fig-align='center' width=100%}\n:::\n\n::: {.cell-output-display}\n![SHAP values for individual prediction](09-advanced-modeling_files/figure-html/fig-individual-predictions-2.png){#fig-individual-predictions-2 fig-align='center' width=100%}\n:::\n:::\n\n\n::: {.callout-note}\n## Code Explanation\n\nIndividual prediction explanations help understand specific cases:\n\n1. **Break-Down Plots**\n   - Show step-by-step how each variable contributes to the prediction\n   - Start from the average prediction and add each variable's effect\n   - Order matters: variables are added in order of importance\n\n2. **SHAP Values**\n   - Shapley Additive exPlanations from game theory\n   - Average contribution across all possible orderings of variables\n   - More robust than break-down plots but computationally intensive\n   - Provides fair attribution of prediction to each feature\n\n3. **Ecological Applications**\n   - Explain why a particular species is predicted in a certain location\n   - Understand which factors drive high/low abundance predictions\n   - Communicate model reasoning to stakeholders and decision-makers\n   - Identify data quality issues or model failures\n:::\n\n::: {.callout-important}\n## Communicating ML Results in Ecology\n\nWhen presenting machine learning results to ecological audiences:\n\n1. **Focus on Ecological Meaning**\n   - Translate statistical importance to ecological significance\n   - Connect model patterns to known ecological processes\n   - Discuss biological plausibility of relationships\n   - Relate findings to conservation implications\n\n2. **Visualize Effectively**\n   - Use partial dependence plots for main effects\n   - Show uncertainty in predictions\n   - Include actual data points alongside model predictions\n   - Use colors and labels that resonate with ecological context\n\n3. **Acknowledge Limitations**\n   - Discuss what the model cannot capture (e.g., biotic interactions)\n   - Explain extrapolation risks\n   - Mention data quality and sampling bias issues\n   - Suggest validation approaches\n\n4. **Provide Actionable Insights**\n   - Link predictions to management recommendations\n   - Identify key variables that can be monitored or manipulated\n   - Suggest priority areas for conservation action\n   - Propose adaptive management strategies based on model uncertainty\n:::\n\nModel interpretation transforms \"black box\" machine learning into a tool for ecological understanding. By explaining how models make predictions, we gain insights into ecological processes and build confidence in using ML for conservation decision-making.\n\n## Time Series Analysis for Ecological Data\n\nTime series data—observations collected sequentially over time—are common in ecology. Population counts, climate measurements, and phenological observations all have temporal structure that standard regression methods cannot properly handle.\n\n### Understanding Temporal Autocorrelation\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load packages for time series analysis\nlibrary(forecast)\nlibrary(tsibble)\nlibrary(feasts)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Create simulated population time series\nset.seed(2024)\nyears <- 1990:2023\nn_years <- length(years)\n\n# Simulate population with trend, seasonality, and noise\ntrend <- 100 + 2 * (1:n_years)  # Increasing trend\nseasonal <- 10 * sin(2 * pi * (1:n_years) / 5)  # 5-year cycle\nnoise <- rnorm(n_years, 0, 5)\npopulation <- trend + seasonal + noise\n\n# Create time series data frame\npop_ts_data <- data.frame(\n  year = years,\n  population = round(population)\n)\n\n# Visualize the time series\nggplot(pop_ts_data, aes(x = year, y = population)) +\n  geom_line(color = \"steelblue\", linewidth = 1) +\n  geom_point(color = \"steelblue\", size = 2) +\n  labs(\n    title = \"Simulated Population Time Series\",\n    subtitle = \"Shows trend and cyclic pattern typical of ecological data\",\n    x = \"Year\",\n    y = \"Population Size\"\n  ) +\n  theme_minimal()\n\n# Test for temporal autocorrelation\n# Create ACF and PACF plots\npar(mfrow = c(2, 1))\nacf(pop_ts_data$population, main = \"Autocorrelation Function (ACF)\")\npacf(pop_ts_data$population, main = \"Partial Autocorrelation Function (PACF)\")\n```\n\n::: {.cell-output-display}\n![Simulated population time series](09-advanced-modeling_files/figure-html/fig-time-series-intro-1.png){#fig-time-series-intro-1 fig-align='center' width=100%}\n:::\n\n::: {.cell-output-display}\n![Autocorrelation and partial autocorrelation functions](09-advanced-modeling_files/figure-html/fig-time-series-intro-2.png){#fig-time-series-intro-2 fig-align='center' width=100%}\n:::\n:::\n\n\n::: {.callout-note}\n## Code Explanation\n\nThis code introduces time series concepts for ecological data:\n\n1. **Time Series Components**\n   - **Trend**: Long-term increase or decrease (e.g., climate change effects)\n   - **Seasonality**: Regular cyclic patterns (e.g., annual breeding cycles)\n   - **Noise**: Random variation (e.g., environmental stochasticity)\n\n2. **Autocorrelation Functions**\n   - **ACF**: Shows correlation between observations at different time lags\n   - **PACF**: Shows direct correlation after removing indirect effects\n   - Significant spikes indicate temporal dependence\n\n3. **Ecological Relevance**\n   - Population dynamics often show autocorrelation due to age structure\n   - Climate variables have strong seasonal patterns\n   - Ignoring autocorrelation leads to invalid statistical inferences\n:::\n\n### Time Series Decomposition\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Convert to ts object for decomposition\npop_ts <- ts(pop_ts_data$population, start = 1990, frequency = 1)\n\n# Decompose the time series\ndecomp <- stl(ts(pop_ts_data$population, frequency = 5), s.window = \"periodic\")\n\n# Plot decomposition\nplot(decomp, main = \"Time Series Decomposition\")\n\n# Extract components\ntrend_component <- decomp$time.series[, \"trend\"]\nseasonal_component <- decomp$time.series[, \"seasonal\"]\nremainder <- decomp$time.series[, \"remainder\"]\n\n# Create a data frame for ggplot\ndecomp_df <- data.frame(\n  year = years,\n  observed = pop_ts_data$population,\n  trend = as.numeric(trend_component),\n  seasonal = as.numeric(seasonal_component),\n  remainder = as.numeric(remainder)\n)\n\n# Visualize components with ggplot\nlibrary(tidyr)\ndecomp_long <- decomp_df %>%\n  pivot_longer(cols = c(observed, trend, seasonal, remainder),\n               names_to = \"component\",\n               values_to = \"value\")\n\nggplot(decomp_long, aes(x = year, y = value)) +\n  geom_line(color = \"steelblue\") +\n  facet_wrap(~component, scales = \"free_y\", ncol = 1) +\n  labs(\n    title = \"Time Series Decomposition Components\",\n    x = \"Year\",\n    y = \"Value\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![Time series decomposition](09-advanced-modeling_files/figure-html/fig-time-series-decomposition-1.png){#fig-time-series-decomposition-1 fig-align='center' width=100%}\n:::\n\n::: {.cell-output-display}\n![Decomposition components visualization](09-advanced-modeling_files/figure-html/fig-time-series-decomposition-2.png){#fig-time-series-decomposition-2 fig-align='center' width=100%}\n:::\n:::\n\n\n::: {.callout-important}\n## Results Interpretation\n\nTime series decomposition reveals the structure of temporal data:\n\n1. **Trend Component**\n   - Shows the long-term direction of the population\n   - Increasing trend suggests population growth\n   - Can be linear or non-linear\n   - Important for long-term conservation planning\n\n2. **Seasonal Component**\n   - Reveals cyclic patterns in the data\n   - In this example, shows a 5-year cycle\n   - Could represent environmental cycles (e.g., El Niño)\n   - Helps identify optimal monitoring times\n\n3. **Remainder (Residuals)**\n   - Random variation after removing trend and seasonality\n   - Should be approximately white noise if decomposition is appropriate\n   - Large residuals may indicate unusual events or data quality issues\n\n4. **Ecological Applications**\n   - Separate long-term trends from natural cycles\n   - Identify critical periods in species life cycles\n   - Detect anomalies or regime shifts\n   - Inform sampling design for monitoring programs\n:::\n\n### Forecasting with ARIMA Models\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Fit an ARIMA model\n# auto.arima selects the best model automatically\narima_model <- auto.arima(pop_ts)\n\n# Display model summary\nsummary(arima_model)\n#> Series: pop_ts \n#> ARIMA(4,1,0) with drift \n#> \n#> Coefficients:\n#>           ar1      ar2      ar3      ar4   drift\n#>       -0.6980  -0.6483  -0.8834  -0.4754  1.8761\n#> s.e.   0.1499   0.1160   0.1091   0.1491  0.3112\n#> \n#> sigma^2 = 46.02:  log likelihood = -109.02\n#> AIC=230.04   AICc=233.27   BIC=239.02\n#> \n#> Training set error measures:\n#>                      ME     RMSE     MAE        MPE     MAPE      MASE\n#> Training set -0.3425243 6.156046 4.88382 -0.5880105 3.756309 0.5052228\n#>                     ACF1\n#> Training set -0.02601765\n\n# Make forecasts for the next 5 years\nforecast_result <- forecast(arima_model, h = 5)\n\n# Plot the forecast\nautoplot(forecast_result) +\n  labs(\n    title = \"Population Forecast (ARIMA Model)\",\n    subtitle = \"5-year forecast with 80% and 95% confidence intervals\",\n    x = \"Year\",\n    y = \"Population Size\"\n  ) +\n  theme_minimal()\n\n# Extract forecast values\nforecast_df <- data.frame(\n  year = 2024:2028,\n  point_forecast = as.numeric(forecast_result$mean),\n  lower_80 = as.numeric(forecast_result$lower[, 1]),\n  upper_80 = as.numeric(forecast_result$upper[, 1]),\n  lower_95 = as.numeric(forecast_result$lower[, 2]),\n  upper_95 = as.numeric(forecast_result$upper[, 2])\n)\n\nprint(forecast_df)\n#>   year point_forecast lower_80 upper_80 lower_95 upper_95\n#> 1 2024       167.6379 158.9443 176.3314 154.3422 180.9335\n#> 2 2025       175.1418 166.0603 184.2232 161.2529 189.0307\n#> 3 2026       173.7157 164.5521 182.8793 159.7011 187.7303\n#> 4 2027       171.1390 161.8463 180.4317 156.9270 185.3509\n#> 5 2028       172.9301 163.3007 182.5595 158.2032 187.6570\n```\n\n::: {.cell-output-display}\n![Population forecast with ARIMA model](09-advanced-modeling_files/figure-html/fig-arima-forecast-1.png){#fig-arima-forecast fig-align='center' width=100%}\n:::\n:::\n\n\n::: {.callout-tip}\n## PROFESSIONAL TIP: Time Series Forecasting in Ecology\n\nWhen forecasting ecological time series:\n\n1. **Model Selection**\n   - Use `auto.arima()` for automatic model selection\n   - Consider seasonal ARIMA for data with clear seasonality\n   - Evaluate multiple models and compare AIC/BIC\n   - Check residual diagnostics to validate model assumptions\n\n2. **Forecast Interpretation**\n   - Point forecasts are the expected values\n   - Confidence intervals quantify uncertainty\n   - Uncertainty increases with forecast horizon\n   - Consider biological constraints (e.g., populations can't be negative)\n\n3. **Ecological Considerations**\n   - Short-term forecasts are more reliable than long-term\n   - Regime shifts or environmental changes can invalidate models\n   - Incorporate external covariates when available (e.g., climate indices)\n   - Use ensemble forecasts for robust predictions\n\n4. **Communication**\n   - Always show uncertainty in forecasts\n   - Explain assumptions and limitations\n   - Relate forecasts to management thresholds\n   - Update forecasts as new data become available\n:::\n\n### Ecological Example: Phenology Trends\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(lmtest)  # For dwtest()\n\n# Simulate phenological data (e.g., first flowering date)\nset.seed(123)\nyears_pheno <- 1980:2023\nn_years_pheno <- length(years_pheno)\n\n# Earlier flowering over time due to climate change\nflowering_doy <- 120 - 0.3 * (1:n_years_pheno) + rnorm(n_years_pheno, 0, 3)\n\npheno_data <- data.frame(\n  year = years_pheno,\n  flowering_day = round(flowering_doy)\n)\n\n# Visualize the trend\nggplot(pheno_data, aes(x = year, y = flowering_day)) +\n  geom_point(color = \"darkgreen\", size = 2) +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE) +\n  labs(\n    title = \"Phenological Shift: First Flowering Date\",\n    subtitle = \"Earlier flowering over time suggests climate warming\",\n    x = \"Year\",\n    y = \"Day of Year (First Flowering)\"\n  ) +\n  theme_minimal()\n\n# Fit a linear model to quantify the trend\npheno_model <- lm(flowering_day ~ year, data = pheno_data)\nsummary(pheno_model)\n#> \n#> Call:\n#> lm(formula = flowering_day ~ year, data = pheno_data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -5.7254 -1.6662 -0.2123  1.7907  6.1424 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 719.0325    66.8573  10.755 1.22e-13 ***\n#> year         -0.3026     0.0334  -9.059 1.97e-11 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.814 on 42 degrees of freedom\n#> Multiple R-squared:  0.6615,\tAdjusted R-squared:  0.6534 \n#> F-statistic: 82.07 on 1 and 42 DF,  p-value: 1.967e-11\n\n# Calculate the rate of change\nrate_per_decade <- coef(pheno_model)[\"year\"] * 10\ncat(sprintf(\"Flowering is occurring %.2f days earlier per decade\\n\", abs(rate_per_decade)))\n#> Flowering is occurring 3.03 days earlier per decade\n\n# Test for autocorrelation in residuals\ndwtest(pheno_model)\n#> \n#> \tDurbin-Watson test\n#> \n#> data:  pheno_model\n#> DW = 1.9126, p-value = 0.3252\n#> alternative hypothesis: true autocorrelation is greater than 0\n\n# If autocorrelation is present, use GLS\nlibrary(nlme)\ngls_model <- gls(flowering_day ~ year,\n                 correlation = corAR1(form = ~ year),\n                 data = pheno_data)\nsummary(gls_model)\n#> Generalized least squares fit by REML\n#>   Model: flowering_day ~ year \n#>   Data: pheno_data \n#>        AIC      BIC  logLik\n#>   226.6999 233.6506 -109.35\n#> \n#> Correlation Structure: AR(1)\n#>  Formula: ~year \n#>  Parameter estimate(s):\n#>        Phi \n#> 0.03243688 \n#> \n#> Coefficients:\n#>                Value Std.Error   t-value p-value\n#> (Intercept) 717.3361  69.01309 10.394204       0\n#> year         -0.3018   0.03448 -8.751694       0\n#> \n#>  Correlation: \n#>      (Intr)\n#> year -1    \n#> \n#> Standardized residuals:\n#>         Min          Q1         Med          Q3         Max \n#> -2.03150156 -0.59213522 -0.07665248  0.63090321  2.17206766 \n#> \n#> Residual standard error: 2.818014 \n#> Degrees of freedom: 44 total; 42 residual\n```\n\n::: {.cell-output-display}\n![Phenological shift: First flowering date over time](09-advanced-modeling_files/figure-html/fig-phenology-trends-1.png){#fig-phenology-trends fig-align='center' width=100%}\n:::\n:::\n\n\n::: {.callout-important}\n## Phenological Trends and Climate Change\n\nPhenological shifts provide powerful evidence of climate change impacts:\n\n1. **Trend Detection**\n   - Linear regression can detect long-term trends\n   - Rate of change quantifies the magnitude of shifts\n   - Statistical significance indicates confidence in the trend\n\n2. **Temporal Autocorrelation**\n   - Phenological data often show autocorrelation\n   - Durbin-Watson test detects autocorrelation in residuals\n   - Generalized Least Squares (GLS) accounts for autocorrelation\n   - Proper modeling prevents inflated Type I error rates\n\n3. **Ecological Implications**\n   - Earlier flowering can lead to phenological mismatches\n   - Pollinators may not be active when plants flower\n   - Affects reproductive success and population dynamics\n   - Indicates ecosystem-wide climate change responses\n\n4. **Conservation Applications**\n   - Monitor phenological shifts as climate change indicators\n   - Identify species vulnerable to phenological mismatches\n   - Inform assisted migration decisions\n   - Guide adaptive management strategies\n:::\n\n::: {.callout-note}\n## Code Explanation\n\nThe phenology example demonstrates practical time series analysis:\n\n1. **Trend Analysis**\n   - Linear regression quantifies the rate of phenological shift\n   - Confidence intervals indicate precision of the estimate\n   - Visual inspection confirms the trend pattern\n\n2. **Autocorrelation Testing**\n   - Durbin-Watson test checks for temporal autocorrelation\n   - Significant autocorrelation requires specialized models\n   - GLS with AR(1) correlation structure accounts for autocorrelation\n\n3. **Model Comparison**\n   - Compare standard linear model with GLS\n   - GLS provides more accurate standard errors\n   - Prevents false positives in trend detection\n:::\n\nTime series analysis is essential for understanding temporal dynamics in ecological systems. By properly accounting for temporal structure, we can detect trends, make forecasts, and understand how ecosystems respond to environmental change.\n\n## Chapter Summary\n\n### Key Concepts\n\n- **Model Interpretation**: DALEX framework provides model-agnostic interpretation methods\n- **Variable Importance**: Permutation importance quantifies predictor contributions\n- **Partial Dependence**: Shows average marginal effects of predictors\n- **ICE Plots**: Reveal individual-level heterogeneity in predictions\n- **SHAP Values**: Provide fair attribution of predictions to features\n- **Temporal Autocorrelation**: Nearby time points are often correlated\n- **Time Series Decomposition**: Separates trend, seasonal, and noise components\n- **ARIMA Models**: Flexible framework for time series forecasting\n- **GLS Models**: Account for temporal autocorrelation in regression\n\n### R Functions Learned\n\n- `explain_tidymodels()` - Create DALEX explainer for tidymodels\n- `model_parts()` - Calculate variable importance\n- `model_profile()` - Create partial dependence and ICE plots\n- `predict_parts()` - Explain individual predictions\n- `auto.arima()` - Automatic ARIMA model selection\n- `forecast()` - Generate forecasts\n## Introduction\n\nThis chapter explores advanced modeling techniques that extend beyond traditional regression (covered in [Chapter 8](08-regression.qmd)). We'll cover model interpretation methods that help us understand \"black box\" machine learning models, and time series analysis for data collected sequentially over time.\n\n### Next Steps\n\nCongratulations on completing *Data Analysis in Natural Sciences*! You now possess a comprehensive toolkit for analyzing ecological data, from data cleaning and visualization to advanced modeling and interpretation.\n\nTo continue your journey:\n-   Apply these techniques to your own datasets\n-   Explore the documentation for packages like `tidymodels`, `DALEX`, and `fable`\n-   Join the R for Data Science community to keep learning\n-   Contribute to open-source projects in ecology and conservation\n\nFor applied examples in conservation science, check out [Chapter 10: Conservation Applications](10-conservation.qmd).\n\n\n## Exercises\n\n1. **Model Interpretation**: Use DALEX to interpret a random forest model predicting species abundance. Which environmental variables are most important?\n\n2. **Partial Dependence**: Create partial dependence plots for the top 3 most important variables. Do the relationships make ecological sense?\n\n3. **Individual Predictions**: Select an unusual prediction (outlier) and use SHAP values to explain why the model made that prediction.\n\n4. **Time Series Decomposition**: Analyze a real ecological time series (e.g., population counts, temperature data). Decompose it and interpret the components.\n\n5. **ARIMA Forecasting**: Build an ARIMA model for a population time series. How far into the future can you reliably forecast?\n\n6. **Phenology Analysis**: Analyze phenological data (e.g., bird arrival dates, flowering times) for evidence of climate change. Account for temporal autocorrelation.\n\n7. **Model Comparison**: Compare interpretations from different ML models (random forest, gradient boosting, neural network) on the same dataset. Do they agree on variable importance?\n\n8. **Ensemble Interpretation**: Create an ensemble model and interpret it. How does the interpretation differ from individual models?\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}