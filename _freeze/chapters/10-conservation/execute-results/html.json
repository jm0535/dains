{
  "hash": "06c80112ecaa269de05635656bf50e03",
  "result": {
    "engine": "knitr",
    "markdown": "---\nprefer-html: true\n---\n\n# Conservation Applications\n\n## Introduction\n\n::: {.callout-note}\n## Learning Objectives\n\nBy the end of this chapter, you will be able to:\n\n1.  Apply statistical methods to assess biodiversity and species richness\n2.  Analyze population trends using time series data and regression models\n3.  Evaluate the effectiveness of conservation interventions (e.g., protected areas)\n4.  Conduct threat assessments using multi-criteria decision analysis\n5.  Interpret statistical results in the context of conservation management\n6.  Communicate data-driven recommendations to stakeholders\n:::\n\nThis chapter explores how data analysis techniques can be applied to conservation science and management. We'll examine how the statistical methods covered in previous chapters (including the advanced modeling techniques in [Chapter 9](09-advanced-modeling.qmd)) can help address real-world conservation challenges, from monitoring endangered species to evaluating the effectiveness of protected areas.\n\n::: {.callout-tip}\n## PROFESSIONAL TIP: Data-Driven Decision Making in Conservation\n\nWhen applying statistical methods to conservation problems:\n\n- **Document analytical decisions**: Clearly explain why you chose specific statistical approaches (e.g., Type II ANOVA for unbalanced ecological data)\n- **Consider scale mismatches**: Ensure your analysis scale matches both ecological processes and management decisions\n- **Acknowledge uncertainty**: Always communicate confidence intervals and limitations of your models to decision-makers\n- **Use multiple lines of evidence**: Combine different analytical approaches to strengthen conservation recommendations\n- **Incorporate local knowledge**: Integrate traditional ecological knowledge with statistical analyses\n- **Apply adaptive management**: Design analyses to evaluate interventions and inform iterative improvements\n- **Consider statistical power**: Ensure monitoring programs have sufficient sample sizes to detect biologically meaningful changes\n- **Report effect sizes**: Focus on magnitude of effects, not just statistical significance\n- **Create accessible visualizations**: Develop clear graphics that communicate results to diverse stakeholders\n- **Archive data and code**: Maintain reproducible workflows that allow others to build on your conservation research\n:::\n\n## Conservation Data Types and Sources\n\n### Types of Conservation Data\n\nConservation science relies on various types of data:\n\n1. **Species Occurrence Data**: Presence/absence or abundance of species\n2. **Habitat Data**: Vegetation structure, land cover, habitat quality\n3. **Threat Data**: Pollution levels, invasive species, human disturbance\n4. **Protected Area Data**: Boundaries, management activities, effectiveness\n5. **Socioeconomic Data**: Human population, land use, resource extraction\n\n### Data Sources\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\nTable: Common Data Sources in Conservation Science\n\n|Source                   |Description                                                                                      |Advantages                                      |Limitations                                                |\n|:------------------------|:------------------------------------------------------------------------------------------------|:-----------------------------------------------|:----------------------------------------------------------|\n|Field Surveys            |Direct collection of data through field observations and measurements                            |High accuracy, detailed information             |Time-consuming, expensive, limited spatial coverage        |\n|Remote Sensing           |Satellite imagery, aerial photography, LiDAR, and other remote sensing techniques                |Large spatial coverage, temporal consistency    |Lower resolution for some applications, cloud cover issues |\n|Citizen Science          |Data collected by volunteers and non-specialists                                                 |Cost-effective, large-scale data collection     |Variable data quality, sampling bias                       |\n|Existing Databases       |GBIF, IUCN Red List, World Database on Protected Areas (WDPA)                                    |Comprehensive, standardized data                |May have gaps, outdated information                        |\n|Environmental Monitoring |Continuous monitoring of environmental variables (e.g., weather stations, water quality sensors) |Continuous temporal data, real-time information |Equipment failures, limited spatial coverage               |\n\n\n:::\n:::\n\n\n## Species Distribution Modeling\n\nSpecies distribution models (SDMs) predict where species are likely to occur based on environmental variables [@elith2009species].\n\n### Example: Simple Species Distribution Model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load required packages\nlibrary(ggplot2)\n\n# Create a simulated environmental dataset\nset.seed(123)\nn <- 200\ntemperature <- runif(n, 5, 30)\nprecipitation <- runif(n, 200, 2000)\nelevation <- runif(n, 0, 3000)\n\n# Calculate species probability based on environmental preferences\n# This species prefers moderate temperatures, high precipitation, and lower elevations\nprobability <- dnorm(temperature, mean = 18, sd = 5) *\n               dnorm(precipitation, mean = 1500, sd = 400) *\n               (1 - elevation/3000)\nprobability <- probability / max(probability)  # Scale to 0-1\n\n# Generate presence/absence based on probability\npresence <- rbinom(n, 1, probability)\n\n# Create a data frame\nspecies_data <- data.frame(\n  temperature = temperature,\n  precipitation = precipitation,\n  elevation = elevation,\n  probability = probability,\n  presence = factor(presence, labels = c(\"Absent\", \"Present\"))\n)\n\n# Visualize the relationship between environmental variables and species presence\nggplot(species_data, aes(x = temperature, y = precipitation, color = presence)) +\n  geom_point(size = 3, alpha = 0.7) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  labs(title = \"Species Presence in Environmental Space\",\n       x = \"Temperature (°C)\",\n       y = \"Precipitation (mm)\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](10-conservation_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=100%}\n:::\n\n```{.r .cell-code}\n\n# Fit a logistic regression model (simple SDM)\nsdm <- glm(presence ~ temperature + precipitation + elevation,\n           family = binomial, data = species_data)\n\n# Summary of the model\nsummary(sdm)\n#> \n#> Call:\n#> glm(formula = presence ~ temperature + precipitation + elevation, \n#>     family = binomial, data = species_data)\n#> \n#> Coefficients:\n#>                 Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)   -2.8652062  0.9176119  -3.122 0.001793 ** \n#> temperature   -0.0073130  0.0317987  -0.230 0.818108    \n#> precipitation  0.0022744  0.0004514   5.039 4.69e-07 ***\n#> elevation     -0.0009398  0.0002641  -3.558 0.000374 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 200.16  on 199  degrees of freedom\n#> Residual deviance: 150.01  on 196  degrees of freedom\n#> AIC: 158.01\n#> \n#> Number of Fisher Scoring iterations: 5\n\n# Calculate predicted probabilities\nspecies_data$predicted <- predict(sdm, type = \"response\")\n\n# Create a prediction surface for visualization\ntemp_seq <- seq(min(temperature), max(temperature), length.out = 50)\nprecip_seq <- seq(min(precipitation), max(precipitation), length.out = 50)\nelev_mean <- mean(elevation)\n\nprediction_grid <- expand.grid(\n  temperature = temp_seq,\n  precipitation = precip_seq,\n  elevation = elev_mean\n)\n\nprediction_grid$probability <- predict(sdm, newdata = prediction_grid, type = \"response\")\n\n# Plot the prediction surface\nggplot(prediction_grid, aes(x = temperature, y = precipitation, fill = probability)) +\n  geom_tile() +\n  scale_fill_viridis_c(option = \"plasma\") +\n  labs(title = \"Predicted Species Distribution\",\n       subtitle = \"Based on temperature and precipitation (at mean elevation)\",\n       x = \"Temperature (°C)\",\n       y = \"Precipitation (mm)\",\n       fill = \"Probability\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](10-conservation_files/figure-html/unnamed-chunk-2-2.png){fig-align='center' width=100%}\n:::\n\n```{.r .cell-code}\n\n# Add actual presence points to the prediction map\nggplot(prediction_grid, aes(x = temperature, y = precipitation, fill = probability)) +\n  geom_tile() +\n  geom_point(data = species_data[species_data$presence == \"Present\", ],\n             aes(x = temperature, y = precipitation),\n             color = \"white\", size = 2, shape = 21) +\n  scale_fill_viridis_c(option = \"plasma\") +\n  labs(title = \"Predicted Species Distribution with Presence Points\",\n       subtitle = \"Based on temperature and precipitation (at mean elevation)\",\n       x = \"Temperature (°C)\",\n       y = \"Precipitation (mm)\",\n       fill = \"Probability\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](10-conservation_files/figure-html/unnamed-chunk-2-3.png){fig-align='center' width=100%}\n:::\n:::\n\n\n::: {.callout-note}\n## Code Explanation\n\nThis code demonstrates how to create a basic Species Distribution Model (SDM) using simulated environmental data. Key components include:\n\n1. **Data Simulation**\n   - We generate simulated environmental data for 200 locations with three variables: temperature, precipitation, and elevation.\n   - We incorporate three key components that reflect real-world species distributions:\n     - Species' environmental preferences (moderate temperatures, high precipitation, lower elevations)\n     - Continuous probabilities of occurrence based on these preferences\n     - Binary presence/absence data simulating the stochastic nature of species occurrence\n\n2. **Data Visualization**\n   - The initial visualization plots raw data by species presence/absence in environmental space (temperature vs. precipitation).\n   - Each colored point represents a different location.\n   - This exploratory step helps identify patterns in the species' environmental preferences before modeling.\n   - The visualization reveals both the relationship between environmental variables and species presence.\n\n3. **Model Fitting**\n   - We use a logistic regression model (`glm` with `family = binomial`) to model the relationship between environmental variables and species presence.\n   - This is a simple but effective approach for species distribution modeling, treating presence/absence as a binary response variable.\n   - The model summary provides coefficient estimates, standard errors, and p-values for each environmental variable.\n\n4. **Prediction Surface**\n   - We create a grid of temperature and precipitation values while holding elevation constant at its mean value.\n   - We use the fitted model to predict the probability of species occurrence across this environmental grid.\n   - We visualize these predictions as a continuous surface using `geom_tile()`, with color intensity representing probability.\n   - We overlay the actual presence points on the prediction surface to assess model fit visually.\n\n5. **Model Limitations**\n   - This simple model assumes that environmental variables affect species occurrence independently.\n   - In reality, species distributions are influenced by biotic interactions, dispersal limitations, and historical factors not captured here.\n   - More sophisticated SDMs might include interaction terms, spatial autocorrelation, or mechanistic components.\n   - Validation with independent data is crucial before using SDMs for conservation decision-making.\n:::\n\n::: {.callout-important}\n## Results Interpretation\n\nThe species distribution modeling approach reveals several important insights for conservation:\n\n1. **Habitat Suitability Assessment**\n   - The prediction surface shows where environmental conditions are most suitable for the species.\n   - Areas with high predicted probability (darker colors) represent potential habitat that could be prioritized for conservation.\n   - The model quantifies the species' environmental niche, showing the optimal ranges for temperature and precipitation.\n\n2. **Climate Change Vulnerability**\n   - By understanding a species' environmental preferences, we can project how climate change might affect its distribution.\n   - For example, if temperatures increase, we can use the model to predict how the species' suitable habitat might shift.\n   - This information is crucial for developing climate adaptation strategies for vulnerable species.\n\n3. **Conservation Planning**\n   - SDMs help identify areas for potential reintroductions or translocations based on environmental suitability.\n   - They can guide protected area design by highlighting environmentally suitable areas that may not currently be protected.\n   - Models can identify potential corridors between suitable habitat patches to maintain connectivity.\n\n4. **Model Limitations and Considerations**\n   - This simple model assumes that environmental variables affect species occurrence independently.\n   - In reality, species distributions are influenced by biotic interactions, dispersal limitations, and historical factors not captured here.\n   - More sophisticated SDMs might include interaction terms, spatial autocorrelation, or mechanistic components.\n   - Validation with independent data is crucial before using SDMs for conservation decision-making.\n:::\n\nSDMs represent a powerful tool in the conservation biologist's toolkit, allowing us to translate ecological knowledge into spatial predictions that can directly inform conservation actions and policy.\n\n::: {.callout-tip}\n## Best Practices for Species Distribution Modeling\n\nWhen developing SDMs for conservation applications:\n\n- **Start with clear hypotheses**: Define which environmental factors are likely to influence the species' distribution based on ecological knowledge\n- **Consider sampling bias**: Account for uneven sampling effort in presence data through spatial filtering or bias correction\n- **Validate thoroughly**: Use independent data or cross-validation techniques to assess model performance and transferability\n- **Incorporate uncertainty**: Present prediction intervals or ensemble model outputs to communicate uncertainty in predictions\n- **Consider scale**: Match the resolution of environmental data to the species' ecology and movement patterns\n- **Include biotic interactions**: When possible, incorporate variables representing key competitors, predators, or mutualists\n:::\n\n## Spatial Autocorrelation in Ecological Data\n\nSpatial autocorrelation—the tendency for nearby locations to have similar values—is ubiquitous in ecological data. Ignoring it can lead to inflated Type I error rates and incorrect inferences about ecological processes.\n\n### Understanding Spatial Autocorrelation\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load required packages\nlibrary(spdep)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Create simulated spatial data with autocorrelation\nset.seed(2024)\nn_sites <- 50\n\n# Generate spatial coordinates\ncoords <- data.frame(\n  x = runif(n_sites, 0, 100),\n  y = runif(n_sites, 0, 100)\n)\n\n# Create spatially autocorrelated species abundance\n# Using distance-based autocorrelation\ndist_matrix <- as.matrix(dist(coords))\nspatial_effect <- numeric(n_sites)\n\nfor(i in 1:n_sites) {\n  # Influence from nearby sites (exponential decay with distance)\n  weights <- exp(-dist_matrix[i,] / 20)\n  weights[i] <- 0  # Don't include self\n  spatial_effect[i] <- sum(weights * rnorm(n_sites, 0, 1)) / sum(weights)\n}\n\n# Combine spatial effect with environmental gradient and noise\nenvironmental_gradient <- coords$x / 100\nabundance <- 50 + 30 * environmental_gradient + 20 * spatial_effect + rnorm(n_sites, 0, 5)\nabundance <- pmax(abundance, 0)  # Ensure non-negative\n\n# Create spatial data frame\nspatial_data <- data.frame(\n  site_id = 1:n_sites,\n  x = coords$x,\n  y = coords$y,\n  abundance = round(abundance),\n  environment = environmental_gradient\n)\n\n# Visualize the spatial pattern\nggplot(spatial_data, aes(x = x, y = y, color = abundance, size = abundance)) +\n  geom_point(alpha = 0.7) +\n  scale_color_viridis_c() +\n  scale_size_continuous(range = c(2, 8)) +\n  labs(\n    title = \"Spatially Autocorrelated Species Abundance\",\n    subtitle = \"Nearby sites tend to have similar abundance values\",\n    x = \"X Coordinate (km)\",\n    y = \"Y Coordinate (km)\",\n    color = \"Abundance\",\n    size = \"Abundance\"\n  ) +\n  theme_minimal() +\n  coord_fixed()\n```\n\n::: {.cell-output-display}\n![](10-conservation_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n::: {.callout-note}\n## Code Explanation\n\nThis code demonstrates spatial autocorrelation in ecological data:\n\n1. **Data Simulation**\n   - Creates 50 sampling sites with random spatial coordinates\n   - Generates abundance values influenced by three factors:\n     - Environmental gradient (east-west trend)\n     - Spatial autocorrelation (nearby sites are similar)\n     - Random noise (natural variation)\n\n2. **Spatial Autocorrelation Mechanism**\n   - Uses distance-based weighting with exponential decay\n   - Sites closer together have more similar values\n   - Mimics real ecological processes like dispersal and habitat similarity\n\n3. **Visualization**\n   - Maps show both location and abundance\n   - Color and size represent abundance\n   - Visual clustering indicates spatial autocorrelation\n:::\n\n### Testing for Spatial Autocorrelation\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Convert to sf object for spatial analysis\nspatial_sf <- st_as_sf(spatial_data, coords = c(\"x\", \"y\"))\n\n# Create spatial weights matrix (k-nearest neighbors)\ncoords_matrix <- st_coordinates(spatial_sf)\nknn_weights <- knearneigh(coords_matrix, k = 4)\nnb <- knn2nb(knn_weights)\nlistw <- nb2listw(nb, style = \"W\")\n\n# Calculate Moran's I\nmoran_test <- moran.test(spatial_data$abundance, listw)\nprint(moran_test)\n#> \n#> \tMoran I test under randomisation\n#> \n#> data:  spatial_data$abundance  \n#> weights: listw    \n#> \n#> Moran I statistic standard deviate = 6.369, p-value = 9.51e-11\n#> alternative hypothesis: greater\n#> sample estimates:\n#> Moran I statistic       Expectation          Variance \n#>       0.555004442      -0.020408163       0.008162256\n\n# Calculate Moran's I for different distance classes (correlogram)\n# Create distance-based neighbors\ndist_classes <- seq(0, 50, by = 10)\nmoran_correlogram <- data.frame(\n  distance = numeric(),\n  morans_i = numeric(),\n  p_value = numeric()\n)\n\nfor(i in 1:(length(dist_classes)-1)) {\n  # Create distance-based neighbors for this class\n  dnb <- dnearneigh(coords_matrix, dist_classes[i], dist_classes[i+1])\n\n  # Only calculate if there are neighbors in this distance class\n  if(sum(card(dnb)) > 0) {\n    dlistw <- nb2listw(dnb, style = \"W\", zero.policy = TRUE)\n    moran_i <- moran.test(spatial_data$abundance, dlistw, zero.policy = TRUE)\n\n    moran_correlogram <- rbind(moran_correlogram, data.frame(\n      distance = mean(c(dist_classes[i], dist_classes[i+1])),\n      morans_i = moran_i$estimate[\"Moran I statistic\"],\n      p_value = moran_i$p.value\n    ))\n  }\n}\n\n# Plot the correlogram\nggplot(moran_correlogram, aes(x = distance, y = morans_i)) +\n  geom_line(color = \"steelblue\", linewidth = 1) +\n  geom_point(aes(color = p_value < 0.05), size = 3) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  scale_color_manual(values = c(\"gray70\", \"red\"),\n                     labels = c(\"Not significant\", \"Significant (p < 0.05)\")) +\n  labs(\n    title = \"Spatial Correlogram (Moran's I)\",\n    subtitle = \"Autocorrelation decreases with distance\",\n    x = \"Distance (km)\",\n    y = \"Moran's I\",\n    color = \"Significance\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](10-conservation_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n::: {.callout-important}\n## Results Interpretation\n\nThe spatial autocorrelation analysis reveals critical information for ecological modeling:\n\n1. **Moran's I Test Results**\n   - Positive Moran's I (typically 0.3-0.7) indicates positive spatial autocorrelation\n   - Significant p-value (< 0.05) confirms autocorrelation is not due to chance\n   - This violates the independence assumption of standard statistical tests\n\n2. **Correlogram Interpretation**\n   - Shows how autocorrelation changes with distance\n   - Typically decreases as distance increases\n   - The distance at which autocorrelation becomes non-significant indicates the \"range\" of spatial dependence\n   - This range informs appropriate sampling design and modeling approaches\n\n3. **Ecological Implications**\n   - Spatial autocorrelation can arise from:\n     - Dispersal processes (organisms spread to nearby areas)\n     - Environmental gradients (similar environments are spatially clustered)\n     - Biotic interactions (species interactions create spatial patterns)\n   - Understanding the cause helps inform conservation strategies\n\n4. **Statistical Consequences**\n   - Ignoring spatial autocorrelation leads to:\n     - Underestimated standard errors\n     - Inflated Type I error rates (false positives)\n     - Incorrect confidence in model predictions\n   - Must account for it in statistical models\n:::\n\n### Accounting for Spatial Autocorrelation in Models\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Fit a naive model (ignoring spatial autocorrelation)\nnaive_model <- lm(abundance ~ environment, data = spatial_data)\n\n# Test residuals for spatial autocorrelation\nlm.morantest(naive_model, listw)\n#> \n#> \tGlobal Moran I for regression residuals\n#> \n#> data:  \n#> model: lm(formula = abundance ~ environment, data = spatial_data)\n#> weights: listw\n#> \n#> Moran I statistic standard deviate = -0.24751, p-value = 0.5977\n#> alternative hypothesis: greater\n#> sample estimates:\n#> Observed Moran I      Expectation         Variance \n#>     -0.061442035     -0.039898085      0.007576155\n\n# Fit a spatial lag model (accounts for spatial autocorrelation)\nlibrary(spatialreg)\nspatial_lag_model <- lagsarlm(abundance ~ environment, data = spatial_data, listw = listw)\n\n# Compare models\ncat(\"Naive Model Summary:\\n\")\n#> Naive Model Summary:\nsummary(naive_model)\n#> \n#> Call:\n#> lm(formula = abundance ~ environment, data = spatial_data)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -17.9456  -4.5578  -0.2574   4.1433  18.1979 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   50.010      2.154  23.213  < 2e-16 ***\n#> environment   30.247      3.550   8.521 3.64e-11 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 7.483 on 48 degrees of freedom\n#> Multiple R-squared:  0.602,\tAdjusted R-squared:  0.5937 \n#> F-statistic: 72.61 on 1 and 48 DF,  p-value: 3.636e-11\n\ncat(\"\\n\\nSpatial Lag Model Summary:\\n\")\n#> \n#> \n#> Spatial Lag Model Summary:\nsummary(spatial_lag_model)\n#> \n#> Call:lagsarlm(formula = abundance ~ environment, data = spatial_data, \n#>     listw = listw)\n#> \n#> Residuals:\n#>       Min        1Q    Median        3Q       Max \n#> -17.93910  -4.53665  -0.27062   4.15985  18.22305 \n#> \n#> Type: lag \n#> Coefficients: (asymptotic standard errors) \n#>             Estimate Std. Error z value  Pr(>|z|)\n#> (Intercept)  50.3308    10.6791  4.7130 2.441e-06\n#> environment  30.4222     6.6001  4.6093 4.040e-06\n#> \n#> Rho: -0.0062684, LR test value: 0.00094442, p-value: 0.97548\n#> Asymptotic standard error: 0.20267\n#>     z-value: -0.030929, p-value: 0.97533\n#> Wald statistic: 0.00095658, p-value: 0.97533\n#> \n#> Log likelihood: -170.5599 for lag model\n#> ML residual variance (sigma squared): 53.759, (sigma: 7.3321)\n#> Number of observations: 50 \n#> Number of parameters estimated: 4 \n#> AIC: 349.12, (AIC for lm: 347.12)\n#> LM test for residual autocorrelation\n#> test value: 4.467, p-value: 0.034555\n\n# Visualize residuals\nspatial_data$naive_residuals <- residuals(naive_model)\nspatial_data$spatial_residuals <- residuals(spatial_lag_model)\n\n# Plot naive model residuals\np1 <- ggplot(spatial_data, aes(x = x, y = y, color = naive_residuals, size = abs(naive_residuals))) +\n  geom_point(alpha = 0.7) +\n  scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\", midpoint = 0) +\n  scale_size_continuous(range = c(2, 6)) +\n  labs(\n    title = \"Naive Model Residuals\",\n    subtitle = \"Spatial pattern in residuals indicates unmodeled autocorrelation\",\n    x = \"X Coordinate (km)\",\n    y = \"Y Coordinate (km)\",\n    color = \"Residual\"\n  ) +\n  theme_minimal() +\n  coord_fixed() +\n  theme(legend.position = \"bottom\")\n\n# Plot spatial model residuals\np2 <- ggplot(spatial_data, aes(x = x, y = y, color = spatial_residuals, size = abs(spatial_residuals))) +\n  geom_point(alpha = 0.7) +\n  scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\", midpoint = 0) +\n  scale_size_continuous(range = c(2, 6)) +\n  labs(\n    title = \"Spatial Lag Model Residuals\",\n    subtitle = \"Reduced spatial pattern indicates better model fit\",\n    x = \"X Coordinate (km)\",\n    y = \"Y Coordinate (km)\",\n    color = \"Residual\"\n  ) +\n  theme_minimal() +\n  coord_fixed() +\n  theme(legend.position = \"bottom\")\n\n# Display plots\nlibrary(patchwork)\np1 / p2\n```\n\n::: {.cell-output-display}\n![](10-conservation_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n::: {.callout-tip}\n## PROFESSIONAL TIP: Dealing with Spatial Autocorrelation\n\nWhen analyzing spatially structured ecological data:\n\n1. **Detection**\n   - Always test for spatial autocorrelation before analysis\n   - Use Moran's I or Geary's C for global autocorrelation\n   - Create correlograms to understand spatial scale\n   - Examine residual plots for spatial patterns\n\n2. **Modeling Approaches**\n   - **Spatial lag models**: When autocorrelation is due to diffusion/dispersal processes\n   - **Spatial error models**: When autocorrelation is due to unmeasured environmental variables\n   - **Generalized Least Squares (GLS)**: Flexible approach for various correlation structures\n   - **Spatial random effects**: Mixed models with spatial random effects\n\n3. **Study Design**\n   - Space sampling sites beyond the range of autocorrelation when possible\n   - Use stratified random sampling to ensure coverage of environmental gradients\n   - Consider spatial autocorrelation in power analyses\n   - Document spatial coordinates for all samples\n\n4. **Reporting**\n   - Report Moran's I and significance tests\n   - Show correlograms to illustrate spatial scale\n   - Compare models with and without spatial structure\n   - Discuss ecological mechanisms causing autocorrelation\n:::\n\nSpatial autocorrelation is not just a statistical nuisance—it provides valuable information about ecological processes. By properly accounting for it, we improve both the validity of our inferences and our understanding of the mechanisms structuring ecological communities.\n\n\n\n## Population Trend Analysis\n\nAnalyzing population trends is crucial for conservation planning and evaluating management effectiveness.\n\n### Example: Linear Mixed Models for Population Trends\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Simulate population monitoring data\nset.seed(456)\nn_sites <- 10\nn_years <- 15\n\n# Create site and year variables\nsite <- rep(paste0(\"Site\", 1:n_sites), each = n_years)\nyear <- rep(2008:(2008 + n_years - 1), times = n_sites)\n\n# Create random site effects and declining trend\nsite_effect <- rep(rnorm(n_sites, 0, 0.5), each = n_years)\ntime_effect <- -0.05 * (year - 2008)  # Declining trend\nnoise <- rnorm(n_sites * n_years, 0, 0.2)\n\n# Calculate log population size\nlog_pop_size <- 2 + site_effect + time_effect + noise\n\n# Convert to actual counts\npopulation <- round(exp(log_pop_size))\n\n# Create a data frame\npop_data <- data.frame(\n  site = factor(site),\n  year = year,\n  population = population\n)\n\n# Visualize the data\nlibrary(ggplot2)\nggplot(pop_data, aes(x = year, y = population, color = site, group = site)) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"Population Trends Across Multiple Sites\",\n       x = \"Year\",\n       y = \"Population Size\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](10-conservation_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=100%}\n:::\n\n```{.r .cell-code}\n\n# Fit a linear mixed model\nlibrary(lme4)\ntrend_model <- lmer(log(population) ~ year + (1|site), data = pop_data)\n\n# Display model summary\nsummary(trend_model)\n#> Linear mixed model fit by REML ['lmerMod']\n#> Formula: log(population) ~ year + (1 | site)\n#>    Data: pop_data\n#> \n#> REML criterion at convergence: 2\n#> \n#> Scaled residuals: \n#>      Min       1Q   Median       3Q      Max \n#> -2.64610 -0.69998 -0.02039  0.62219  1.92852 \n#> \n#> Random effects:\n#>  Groups   Name        Variance Std.Dev.\n#>  site     (Intercept) 0.17634  0.4199  \n#>  Residual             0.04223  0.2055  \n#> Number of obs: 150, groups:  site, 10\n#> \n#> Fixed effects:\n#>               Estimate Std. Error t value\n#> (Intercept) 100.003639   7.826672   12.78\n#> year         -0.048800   0.003884  -12.57\n#> \n#> Correlation of Fixed Effects:\n#>      (Intr)\n#> year -1.000\n\n# Calculate overall trend\ntrend_coef <- fixef(trend_model)[\"year\"]\nannual_change <- (exp(trend_coef) - 1) * 100\ncat(\"Annual population change:\", round(annual_change, 2), \"%\\n\")\n#> Annual population change: -4.76 %\n\n# Predict values for visualization\npop_data$predicted <- exp(predict(trend_model))\n\n# Plot observed vs. predicted values\nggplot(pop_data, aes(x = year)) +\n  geom_point(aes(y = population, color = site), alpha = 0.5) +\n  geom_line(aes(y = predicted, group = site), color = \"black\") +\n  labs(title = \"Observed and Predicted Population Sizes\",\n       x = \"Year\",\n       y = \"Population Size\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](10-conservation_files/figure-html/unnamed-chunk-6-2.png){fig-align='center' width=100%}\n:::\n:::\n\n\n::: {.callout-note}\n## Code Explanation\n\nThis code demonstrates how to analyze population trends across multiple monitoring sites using linear mixed models, a powerful approach for conservation monitoring data. Key components include:\n\n1. **Data Simulation**\n   - We simulate 15 years of population monitoring data across 10 different sites.\n   - We incorporate three key components that reflect real-world population dynamics:\n     - Site-specific random effects (some sites naturally support larger populations)\n     - A systematic declining trend over time (the conservation concern)\n     - Random noise (natural population fluctuations)\n   - We use a log-normal model for population size, which is appropriate for count data that can't be negative.\n\n2. **Data Visualization**\n   - The initial visualization plots raw population counts over time for each site.\n   - Each colored line represents a different monitoring site.\n   - This exploratory plot helps identify overall patterns and site-specific variations.\n   - The visualization reveals both the declining trend and the between-site variability.\n\n3. **Model Fitting**\n   - We use a linear mixed model (LMM) with the `lme4` package to analyze the population trend.\n   - We log-transform the population counts to stabilize variance and make the model more appropriate for count data.\n   - The fixed effect (`year`) captures the overall temporal trend shared across all sites.\n   - The random effect (`1|site`) accounts for site-specific variation in baseline population sizes.\n   - This approach is more powerful than analyzing each site separately, as it \"borrows strength\" across sites.\n\n4. **Trend Quantification**\n   - We extract the year coefficient from the model, which represents the average annual change in log population size.\n   - We convert this to a percentage change using the formula `(exp(coef) - 1) * 100`.\n   - This transformation makes the result more interpretable for conservation managers and policymakers.\n\n5. **Model Visualization**\n   - We generate predicted values from the model for each site and year.\n   - We plot both observed data (colored points) and model predictions (black lines).\n   - This helps assess model fit and visualize the estimated trend while accounting for site-specific differences.\n:::\n\n::: {.callout-important}\n## Results Interpretation\n\nThe population trend analysis provides several important insights for conservation management:\n\n1. **Quantifying Population Decline**\n   - The model estimates an annual population decline of approximately 5%, which is substantial and concerning from a conservation perspective.\n   - The statistical significance of this trend (as shown in the model summary) helps determine whether conservation action is warranted.\n   - The confidence interval around this estimate (not explicitly calculated here) would indicate the precision of our trend estimate.\n\n2. **Site-Specific Variation**\n   - The random effects reveal which sites have consistently higher or lower populations than average.\n   - This information can help identify potential refuges (sites with larger populations) or areas of concern (sites with smaller populations).\n   - Understanding site-specific variation is crucial for prioritizing conservation efforts and resources.\n\n3. **Conservation Decision Support**\n   - This analysis provides quantitative evidence to support conservation decisions:\n     - Is the population declining at a rate that requires intervention?\n     - Which sites should be prioritized for management actions?\n     - How much would the population need to increase annually to reach recovery targets?\n\n4. **Monitoring Program Design**\n   - The approach demonstrates the value of multi-site monitoring programs.\n   - The mixed model framework allows detection of trends that might be obscured by site-specific variation.\n   - This can inform the design of future monitoring programs, including the number of sites needed and monitoring frequency.\n\n5. **Limitations and Considerations**\n   - This simple model assumes a constant rate of decline across years.\n   - More complex models might include non-linear trends, temporal autocorrelation, or environmental covariates.\n   - For real conservation applications, additional diagnostics would be needed to validate model assumptions.\n:::\n\nThis mixed modeling approach represents a powerful tool for conservation biologists, allowing them to rigorously assess population trends while accounting for the complex, hierarchical nature of ecological monitoring data.\n\n::: {.callout-tip}\n## Best Practices for Population Trend Analysis\n\nWhen analyzing population trends for conservation decision-making:\n\n- **Use appropriate temporal scale**: Consider the species' generation time and life history when determining monitoring frequency\n- **Account for detection probability**: Imperfect detection can bias trend estimates; use occupancy or N-mixture models when detection is < 100%\n- **Consider environmental covariates**: Including climate or habitat variables can help explain population fluctuations and distinguish natural variation from concerning declines\n- **Report effect sizes, not just p-values**: A statistically significant decline might not be biologically significant; focus on magnitude and uncertainty\n- **Evaluate multiple metrics**: Analyze abundance, occupancy, and demographic rates together for a more complete picture of population health\n- **Plan for statistical power**: Design monitoring programs with enough sites and years to detect trends of conservation concern\n:::\n\n## Habitat Fragmentation Analysis\n\nHabitat fragmentation is a major threat to biodiversity. Landscape metrics help quantify fragmentation patterns.\n\n### Example: Calculating Landscape Metrics\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load required packages\nlibrary(terra)\nlibrary(ggplot2)\n\n# Create a simple landscape raster\nr <- rast(ncol=30, nrow=30)\nvalues(r) <- sample(c(1, 2, 3, 4), ncell(r), replace=TRUE,\n                   prob=c(0.4, 0.3, 0.2, 0.1))\nnames(r) <- \"landcover\"\n\n# Plot the landscape\nplot(r, main=\"Simulated Landscape\", col=c(\"forestgreen\", \"yellow\", \"blue\", \"grey\"))\n```\n\n::: {.cell-output-display}\n![](10-conservation_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=100%}\n:::\n\n```{.r .cell-code}\n\n# Create a data frame with class-level metrics manually\nclass_metrics <- data.frame(\n  class = c(1, 2, 3, 4),\n  class_name = c(\"Forest\", \"Agriculture\", \"Water\", \"Urban\"),\n  percentage = c(40, 30, 20, 10),\n  edge_density = c(0.12, 0.09, 0.06, 0.03),\n  num_patches = c(15, 12, 8, 5)\n)\n\n# Visualize class-level metrics\nggplot(class_metrics, aes(x = factor(class), y = percentage, fill = factor(class))) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Percentage of Landscape by Class\",\n       x = \"Land Cover Class\",\n       y = \"Percentage (%)\") +\n  scale_fill_manual(values = c(\"forestgreen\", \"yellow\", \"blue\", \"grey\"),\n                    labels = class_metrics$class_name) +\n  theme_minimal() +\n  theme(legend.title = element_blank())\n```\n\n::: {.cell-output-display}\n![](10-conservation_files/figure-html/unnamed-chunk-7-2.png){fig-align='center' width=100%}\n:::\n\n```{.r .cell-code}\n\n# Visualize number of patches\nggplot(class_metrics, aes(x = factor(class), y = num_patches, fill = factor(class))) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Number of Patches by Class\",\n       x = \"Land Cover Class\",\n       y = \"Number of Patches\") +\n  scale_fill_manual(values = c(\"forestgreen\", \"yellow\", \"blue\", \"grey\"),\n                    labels = class_metrics$class_name) +\n  theme_minimal() +\n  theme(legend.title = element_blank())\n```\n\n::: {.cell-output-display}\n![](10-conservation_files/figure-html/unnamed-chunk-7-3.png){fig-align='center' width=100%}\n:::\n\n```{.r .cell-code}\n\n# Visualize edge density\nggplot(class_metrics, aes(x = factor(class), y = edge_density, fill = factor(class))) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Edge Density by Class\",\n       x = \"Land Cover Class\",\n       y = \"Edge Density\") +\n  scale_fill_manual(values = c(\"forestgreen\", \"yellow\", \"blue\", \"grey\"),\n                    labels = class_metrics$class_name) +\n  theme_minimal() +\n  theme(legend.title = element_blank())\n```\n\n::: {.cell-output-display}\n![](10-conservation_files/figure-html/unnamed-chunk-7-4.png){fig-align='center' width=100%}\n:::\n:::\n\n\n::: {.callout-note}\n## Code Explanation\n\nThis code demonstrates how to create and analyze a simulated landscape to study habitat fragmentation patterns. Key components include:\n\n1. **Landscape Simulation**\n   - We use the `terra` package to create a 30×30 cell raster representing a landscape.\n   - We randomly assign each cell to one of four land cover classes: Forest (1), Agriculture (2), Water (3), and Urban (4).\n   - The probability distribution (40%, 30%, 20%, 10%) creates a landscape dominated by forest and agricultural land.\n   - This simulated landscape provides a controlled environment to demonstrate fragmentation analysis techniques.\n\n2. **Landscape Visualization**\n   - We visualize the landscape using appropriate colors for each land cover type (green for forest, yellow for agriculture, blue for water, grey for urban).\n   - This spatial representation helps identify patterns of fragmentation visually before quantitative analysis.\n   - The mosaic pattern reveals how different land cover types are distributed and potentially fragmented across the landscape.\n\n3. **Landscape Metrics Calculation**\n   - In a real analysis, metrics would be calculated directly from the raster using packages like `landscapemetrics`.\n   - For this example, we manually create a data frame with three key metrics for each land cover class:\n     - **Percentage**: The proportion of the landscape occupied by each class\n     - **Edge Density**: The amount of edge relative to the landscape area (higher values indicate more fragmentation)\n     - **Number of Patches**: Count of discrete patches for each land cover type (more patches suggest higher fragmentation)\n\n4. **Metrics Visualization**\n   - We create three bar charts to visualize each landscape metric by land cover class.\n   - Consistent color coding across all visualizations helps maintain visual connection to the landscape map.\n   - Each chart focuses on a different aspect of landscape composition and configuration.\n:::\n\n::: {.callout-important}\n## Results Interpretation\n\nThe landscape fragmentation analysis reveals several key insights for conservation planning:\n\n1. **Landscape Composition**\n   - The percentage chart shows that forest covers 40% of the landscape, followed by agriculture (30%), water (20%), and urban areas (10%).\n   - This composition analysis helps establish conservation priorities based on habitat availability.\n   - In real-world applications, comparing this to historical land cover would reveal habitat loss trends.\n\n2. **Fragmentation Assessment**\n   - The number of patches metric reveals that forest (15 patches) is more fragmented than other land cover types.\n   - Despite having the highest coverage, forest fragmentation may compromise its ecological value for species requiring large, continuous habitat.\n   - Urban areas have the fewest patches (5), suggesting they form more concentrated developments.\n\n3. **Edge Effects**\n   - Edge density is highest for forest (0.12), indicating extensive borders with other land cover types.\n   - High edge density creates \"edge effects\" that can negatively impact forest-interior species through:\n     - Altered microclimate conditions (light, temperature, humidity)\n     - Increased predation and nest parasitism\n     - Invasive species introduction\n   - Water has relatively low edge density despite moderate patch numbers, suggesting more compact water bodies.\n\n4. **Conservation Implications**\n   - These metrics help identify specific conservation needs:\n     - **Habitat Connectivity**: Forest patches might need corridors to reconnect fragmented habitats.\n     - **Buffer Zones**: High edge density suggests the need for buffer zones around sensitive habitats.\n     - **Restoration Priorities**: Strategically restoring habitat in areas that would reconnect patches.\n     - **Development Planning**: Guiding future development to minimize additional fragmentation.\n\n5. **Limitations and Considerations**\n   - This simplified example uses a coarse resolution and random distribution.\n   - Real landscapes have spatial autocorrelation and are influenced by topography, hydrology, and human infrastructure.\n   - Additional metrics like connectivity indices, core area, and shape complexity would provide more comprehensive fragmentation assessment.\n   - Scale dependency is important - fragmentation patterns may differ at different spatial resolutions.\n:::\n\nLandscape metrics translate complex spatial patterns into quantifiable measures that conservation biologists can use to assess habitat quality, prioritize conservation efforts, and monitor landscape change over time. These approaches are particularly valuable for addressing habitat fragmentation, one of the primary drivers of biodiversity loss globally.\n\n::: {.callout-tip}\n## Best Practices for Habitat Fragmentation Analysis\n\nWhen analyzing landscape patterns for conservation planning:\n\n- **Consider multiple scales**: Analyze fragmentation at different spatial scales as species respond to landscape structure at different scales\n- **Use ecologically relevant metrics**: Select metrics that relate to the ecological processes and species of interest\n- **Incorporate temporal dynamics**: Monitor landscape changes over time to detect fragmentation trends and evaluate restoration success\n- **Link to biodiversity data**: Correlate landscape metrics with species occurrence or abundance to validate their ecological relevance\n- **Account for matrix quality**: Consider the permeability of the landscape matrix between habitat patches, not just patch characteristics\n- **Combine with connectivity analysis**: Supplement fragmentation metrics with explicit connectivity models to identify critical corridors\n:::\n\n## Protected Area Effectiveness\n\nEvaluating the effectiveness of protected areas is essential for conservation planning and management.\n\n### Example: Before-After-Control-Impact (BACI) Analysis\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Simulate protected area effectiveness data\nset.seed(789)\nn_sites <- 20\nn_years <- 10\n\n# Create site, protection status, and year variables\nsite <- rep(paste0(\"Site\", 1:n_sites), each = n_years)\nprotected <- rep(rep(c(\"Protected\", \"Unprotected\"), each = n_sites/2), each = n_years)\nyear <- rep(2013:(2013 + n_years - 1), times = n_sites)\nperiod <- ifelse(year < 2018, \"Before\", \"After\")  # Protection started in 2018\n\n# Create random site effects and impact of protection\nsite_effect <- rep(rnorm(n_sites, 0, 0.5), each = n_years)\nprotection_effect <- ifelse(protected == \"Protected\" & period == \"After\", 0.3, 0)\ntime_effect <- -0.05 * (year - 2013)  # General declining trend\nnoise <- rnorm(n_sites * n_years, 0, 0.2)\n\n# Calculate biodiversity index\nbiodiversity <- 5 + site_effect + time_effect + protection_effect + noise\n\n# Create a data frame\npa_data <- data.frame(\n  site = factor(site),\n  protected = factor(protected),\n  year = year,\n  period = factor(period),\n  biodiversity = biodiversity\n)\n\n# Visualize the data\nggplot(pa_data, aes(x = year, y = biodiversity, color = protected, group = interaction(site, protected))) +\n  geom_line(alpha = 0.3) +\n  stat_summary(aes(group = protected), fun = mean, geom = \"line\", linewidth = 1.5) +\n  geom_vline(xintercept = 2018, linetype = \"dashed\") +\n  labs(title = \"Biodiversity Trends in Protected and Unprotected Sites\",\n       subtitle = \"Vertical line indicates when protection was implemented\",\n       x = \"Year\",\n       y = \"Biodiversity Index\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](10-conservation_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=100%}\n:::\n\n```{.r .cell-code}\n\n# Fit a BACI model\nbaci_model <- lm(biodiversity ~ protected * period, data = pa_data)\n\n# Display model summary\nsummary(baci_model)\n#> \n#> Call:\n#> lm(formula = biodiversity ~ protected * period, data = pa_data)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -1.18762 -0.25169  0.00786  0.29460  0.93568 \n#> \n#> Coefficients:\n#>                                   Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)                        4.79029    0.05943  80.604  < 2e-16 ***\n#> protectedUnprotected              -0.29698    0.08405  -3.534 0.000511 ***\n#> periodBefore                      -0.08027    0.08405  -0.955 0.340742    \n#> protectedUnprotected:periodBefore  0.33219    0.11886   2.795 0.005709 ** \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.4202 on 196 degrees of freedom\n#> Multiple R-squared:  0.06998,\tAdjusted R-squared:  0.05574 \n#> F-statistic: 4.916 on 3 and 196 DF,  p-value: 0.002578\n\n# Visualize the interaction effect\npa_summary <- aggregate(biodiversity ~ protected + period, data = pa_data, FUN = mean)\n\nggplot(pa_summary, aes(x = period, y = biodiversity, color = protected, group = protected)) +\n  geom_point(size = 3) +\n  geom_line() +\n  labs(title = \"BACI Design: Interaction between Protection Status and Time Period\",\n       x = \"Period\",\n       y = \"Mean Biodiversity Index\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](10-conservation_files/figure-html/unnamed-chunk-8-2.png){fig-align='center' width=100%}\n:::\n:::\n\n\n::: {.callout-note}\n## Code Explanation\n\nThis code demonstrates a Before-After-Control-Impact (BACI) analysis for evaluating protected area effectiveness:\n\n1. **Data Simulation**:\n   - Creates biodiversity monitoring data for 20 sites over 10 years\n   - Half the sites are protected starting in 2018\n   - Includes site-specific random effects, a protection effect, and natural variation\n\n2. **BACI Design Components**:\n   - **Before-After**: Time periods before and after protection implementation\n   - **Control-Impact**: Comparison between protected and unprotected sites\n   - **Interaction**: The key element that tests whether protection made a difference\n\n3. **Visualization Elements**:\n   - Individual site trajectories shown with thin lines\n   - Mean trends highlighted with thicker lines\n   - Vertical line marking when protection was implemented\n   - Interaction plot showing the mean values for each combination\n:::\n\n::: {.callout-important}\n## Results Interpretation\n\nThe BACI analysis reveals crucial information about protected area effectiveness:\n\n1. **Protection Impact**:\n   - The interaction term (protected:periodAfter) shows the true effect of protection\n   - Positive coefficient indicates protection is benefiting biodiversity\n   - Statistical significance of this term determines whether protection is working\n\n2. **Counterfactual Analysis**:\n   - Unprotected sites serve as the counterfactual (what would have happened without protection)\n   - Overall declining trend in both site types indicates broader environmental pressures\n   - Difference in slopes represents the conservation value added by protection\n\n3. **Management Implications**:\n   - Quantifies the return on investment for conservation funding\n   - Helps determine whether current protection strategies are sufficient\n   - Provides evidence for maintaining or expanding protection efforts\n:::\n\n::: {.callout-tip}\n## PROFESSIONAL TIP: Designing Effective BACI Studies\n\nWhen evaluating conservation interventions:\n\n1. **Study Design**:\n   - Select control sites that match impact sites in key environmental variables\n   - Ensure sufficient monitoring before intervention implementation\n   - Include multiple control and impact sites to account for site-specific variation\n   - Consider spatial autocorrelation in site selection\n\n2. **Analysis Approach**:\n   - Use linear mixed models for nested or repeated measures designs\n   - Include relevant covariates that might affect outcomes\n   - Consider temporal autocorrelation in time series data\n   - Test for pre-existing differences between control and impact sites\n\n3. **Interpretation**:\n   - Focus on the interaction term (difference in differences)\n   - Report effect sizes and confidence intervals, not just p-values\n   - Consider time lags in conservation responses\n   - Discuss both statistical and practical significance\n:::\n\n## Threat Assessment and Prioritization\n\nConservation resources are limited, so prioritizing threats and actions is essential.\n\n### Example: Multi-Criteria Decision Analysis\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyr)  # For pivot_longer\n\n# Create a threat assessment dataset\nthreats <- c(\"Habitat Loss\", \"Invasive Species\", \"Climate Change\", \"Pollution\", \"Overexploitation\")\nseverity <- c(0.9, 0.7, 0.8, 0.6, 0.7)\nscope <- c(0.8, 0.6, 0.9, 0.5, 0.6)\nirreversibility <- c(0.9, 0.7, 0.9, 0.4, 0.5)\n\n# Create a data frame\nthreat_data <- data.frame(\n  threat = threats,\n  severity = severity,\n  scope = scope,\n  irreversibility = irreversibility\n)\n\n# Calculate overall threat magnitude\nthreat_data$magnitude <- with(threat_data, severity * scope * irreversibility)\n\n# Sort by magnitude\nthreat_data <- threat_data[order(threat_data$magnitude, decreasing = TRUE), ]\n\n# Visualize the threat assessment\nggplot(threat_data, aes(x = reorder(threat, magnitude), y = magnitude)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(title = \"Threat Prioritization Based on Magnitude\",\n       x = \"Threat\",\n       y = \"Magnitude (Severity × Scope × Irreversibility)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](10-conservation_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=100%}\n:::\n\n```{.r .cell-code}\n\n# Visualize the components (using tidyr for data reshaping)\nthreat_data_long <- threat_data %>%\n  dplyr::select(threat, severity, scope, irreversibility) %>%\n  pivot_longer(cols = -threat, names_to = \"variable\", values_to = \"value\")\n\nggplot(threat_data_long, aes(x = reorder(threat, -value), y = value, fill = variable)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Components of Threat Assessment\",\n       x = \"Threat\",\n       y = \"Score\",\n       fill = \"Component\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](10-conservation_files/figure-html/unnamed-chunk-9-2.png){fig-align='center' width=100%}\n:::\n:::\n\n\n::: {.callout-note}\n## Code Explanation\n\nThis code demonstrates multi-criteria decision analysis for threat prioritization:\n\n1. **Data Structure**:\n   - Creates an example dataset of five common conservation threats\n   - Evaluates each threat using three criteria:\n     - **Severity**: The intensity of the threat's impact\n     - **Scope**: The proportion of the target affected\n     - **Irreversibility**: How difficult it is to reverse the damage\n\n2. **Analysis Process**:\n   - Calculates an overall magnitude score by multiplying the three criteria\n   - Ranks threats based on this composite score\n   - Creates visualizations to compare both overall rankings and component scores\n\n3. **Visualization Techniques**:\n   - Bar chart of overall threat magnitude\n   - Grouped bar chart showing the individual criteria for each threat\n   - Consistent ordering of threats by magnitude\n   - Clear labeling and color-coding\n:::\n\n::: {.callout-important}\n## Results Interpretation\n\nThe threat prioritization analysis reveals important insights for conservation planning:\n\n1. **Threat Ranking**:\n   - Habitat Loss emerges as the highest priority threat\n   - Climate Change ranks second despite its lower severity\n   - Overexploitation has the lowest composite score\n\n2. **Component Analysis**:\n   - Habitat Loss scores consistently high across all three criteria\n   - Climate Change has high scope and irreversibility but slightly lower severity\n   - Pollution shows low irreversibility despite moderate severity and scope\n\n3. **Conservation Implications**:\n   - Resources should be allocated according to threat magnitude\n   - Different threats require different intervention strategies:\n     - For reversible threats: direct mitigation\n     - For irreversible threats: prevention and adaptation\n   - Comprehensive strategies needed for threats scoring high in all dimensions\n:::\n\n::: {.callout-tip}\n## PROFESSIONAL TIP: Effective Threat Assessment\n\nWhen prioritizing conservation threats:\n\n1. **Assessment Process**:\n   - Include diverse stakeholders and experts in evaluations\n   - Define criteria explicitly with clear scoring guidelines\n   - Consider both direct and indirect threats\n   - Document uncertainty in threat evaluations\n\n2. **Analysis Considerations**:\n   - Test sensitivity to different scoring methods and weights\n   - Consider interactions between threats\n   - Evaluate threats at appropriate spatial and temporal scales\n   - Include emerging and potential future threats\n\n3. **Application to Decision-Making**:\n   - Link threat assessment directly to conservation actions\n   - Consider feasibility and cost-effectiveness of addressing each threat\n   - Re-evaluate periodically as conditions change\n   - Communicate results clearly to decision-makers\n:::\n\n## Conservation Planning\n\nSystematic conservation planning helps identify priority areas for conservation.\n\n### Example: Complementarity Analysis\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Create a species-by-site matrix\nset.seed(101)\nn_sites <- 10\nn_species <- 15\nspecies_names <- paste0(\"Species\", 1:n_species)\nsite_names <- paste0(\"Site\", 1:n_sites)\n\n# Generate presence/absence data\npresence_prob <- matrix(runif(n_sites * n_species, 0, 1), nrow = n_sites, ncol = n_species)\npresence <- ifelse(presence_prob > 0.7, 0, 1)  # 30% chance of presence\nrownames(presence) <- site_names\ncolnames(presence) <- species_names\n\n# Calculate species richness per site\nrichness <- rowSums(presence)\n\n# Calculate site complementarity\ncomplementarity <- function(selected, candidates, presence_matrix) {\n  if (length(selected) == 0) {\n    # If no sites selected yet, return site richness\n    return(rowSums(presence_matrix[candidates, , drop = FALSE]))\n  } else {\n    # Calculate new species added by each candidate site\n    species_in_selected <- colSums(presence_matrix[selected, , drop = FALSE]) > 0\n    new_species <- function(site) {\n      sum(presence_matrix[site, ] & !species_in_selected)\n    }\n    return(sapply(candidates, new_species))\n  }\n}\n\n# Greedy algorithm for site selection\nselect_sites <- function(presence_matrix, n_to_select) {\n  n_sites <- nrow(presence_matrix)\n  available_sites <- 1:n_sites\n  selected_sites <- integer(0)\n\n  for (i in 1:n_to_select) {\n    if (length(available_sites) == 0) break\n\n    # Calculate complementarity scores\n    scores <- complementarity(selected_sites, available_sites, presence_matrix)\n\n    # Select site with highest score\n    best <- available_sites[which.max(scores)]\n    selected_sites <- c(selected_sites, best)\n    available_sites <- setdiff(available_sites, best)\n  }\n\n  return(selected_sites)\n}\n\n# Select 3 priority sites\npriority_sites <- select_sites(presence, 3)\ncat(\"Priority sites:\", site_names[priority_sites], \"\\n\")\n#> Priority sites: Site7 Site8 Site1\n\n# Calculate species coverage\nspecies_covered <- colSums(presence[priority_sites, , drop = FALSE]) > 0\ncat(\"Species covered:\", sum(species_covered), \"out of\", n_species,\n    \"(\", round(100 * sum(species_covered) / n_species, 1), \"%)\\n\")\n#> Species covered: 15 out of 15 ( 100 %)\n\n# Visualize the species-site matrix\nlibrary(pheatmap)\npheatmap(presence,\n        cluster_rows = FALSE,\n        cluster_cols = FALSE,\n        main = \"Species Presence by Site\",\n        color = c(\"white\", \"steelblue\"),\n        labels_row = site_names,\n        labels_col = species_names,\n        display_numbers = TRUE,\n        number_color = \"black\",\n        fontsize = 10,\n        fontsize_number = 8)\n```\n\n::: {.cell-output-display}\n![](10-conservation_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=100%}\n:::\n\n```{.r .cell-code}\n\n# Highlight priority sites\npriority_data <- data.frame(\n  Priority = factor(ifelse(1:n_sites %in% priority_sites, \"Selected\", \"Not Selected\"))\n)\nrownames(priority_data) <- site_names\n\npheatmap(presence,\n        cluster_rows = FALSE,\n        cluster_cols = FALSE,\n        main = \"Priority Sites for Conservation\",\n        color = c(\"white\", \"steelblue\"),\n        labels_row = site_names,\n        labels_col = species_names,\n        display_numbers = TRUE,\n        number_color = \"black\",\n        annotation_row = priority_data,\n        fontsize = 10,\n        fontsize_number = 8)\n```\n\n::: {.cell-output-display}\n![](10-conservation_files/figure-html/unnamed-chunk-10-2.png){fig-align='center' width=100%}\n:::\n:::\n\n\n::: {.callout-note}\n## Code Explanation\n\nThis code demonstrates systematic conservation planning using complementarity analysis:\n\n1. **Data Preparation**:\n   - Creates a simulated presence/absence matrix of 15 species across 10 sites\n   - Each cell represents whether a species occurs at a site (1) or not (0)\n   - The matrix represents the kind of data collected during biodiversity surveys\n\n2. **Complementarity Algorithm**:\n   - Implements a greedy algorithm for site selection\n   - First selects the site with highest species richness\n   - Subsequent selections maximize additional species not already protected\n   - This approach efficiently captures maximum biodiversity with minimum sites\n\n3. **Visualization Approach**:\n   - Uses heatmaps to display the species-site matrix\n   - Colors indicate presence (blue) or absence (white)\n   - Highlights selected priority sites with annotation\n   - Displays numerical values within cells for clarity\n:::\n\n::: {.callout-important}\n## Results Interpretation\n\nThe complementarity analysis provides key insights for conservation prioritization:\n\n1. **Efficiency of Site Selection**:\n   - The algorithm selected just 3 sites that protect most (typically >70%) of the species\n   - This demonstrates the efficiency of complementarity-based selection\n   - Traditional approaches might require more sites to achieve the same coverage\n\n2. **Site Prioritization**:\n   - The selected sites represent the most irreplaceable areas for biodiversity\n   - These should be highest priorities for protection or management\n   - The visualization clearly shows which species are protected in each site\n\n3. **Conservation Planning Applications**:\n   - Helps make evidence-based decisions for protected area designation\n   - Maximizes return on investment when conservation resources are limited\n   - Ensures representation of different species rather than just protecting species-rich areas\n:::\n\n::: {.callout-tip}\n## PROFESSIONAL TIP: Effective Conservation Planning\n\nWhen applying complementarity analysis:\n\n1. **Data Considerations**:\n   - Use the most comprehensive species data available\n   - Consider taxonomic, functional, and genetic diversity\n   - Account for data quality issues and sampling bias\n   - Include threatened species with higher weighting if appropriate\n\n2. **Algorithm Selection**:\n   - Simple greedy algorithms work well for small problems\n   - Consider optimization algorithms (e.g., simulated annealing) for complex scenarios\n   - Include connectivity and spatial considerations when possible\n   - Set meaningful conservation targets (e.g., protect 30% of each species' range)\n\n3. **Implementation Strategy**:\n   - Use results to inform both formal protection and other conservation measures\n   - Consider practical constraints like land availability and cost\n   - Engage stakeholders in the planning process\n   - Update analyses as new data becomes available\n:::\n\n## Climate Change Vulnerability Assessment\n\nClimate change poses significant threats to biodiversity. Vulnerability assessments help identify at-risk species and ecosystems.\n\n### Example: Trait-Based Vulnerability Analysis\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Create a species trait dataset\nspecies <- paste0(\"Species\", 1:12)\ndispersal_ability <- c(1, 3, 2, 1, 3, 2, 1, 2, 3, 1, 2, 3)  # 1=low, 2=medium, 3=high\nthermal_tolerance <- c(1, 2, 3, 1, 2, 3, 2, 3, 1, 3, 1, 2)  # 1=low, 2=medium, 3=high\nhabitat_specificity <- c(3, 2, 1, 3, 1, 2, 3, 2, 1, 2, 3, 1)  # 1=low, 2=medium, 3=high\npopulation_size <- c(1, 2, 3, 1, 3, 2, 1, 3, 2, 1, 2, 3)  # 1=small, 2=medium, 3=large\n\n# Create a data frame\nvulnerability_data <- data.frame(\n  species = species,\n  dispersal_ability = dispersal_ability,\n  thermal_tolerance = thermal_tolerance,\n  habitat_specificity = habitat_specificity,\n  population_size = population_size\n)\n\n# Calculate vulnerability scores (higher = more vulnerable)\nvulnerability_data$sensitivity <- 4 - thermal_tolerance\nvulnerability_data$adaptive_capacity <- 4 - (dispersal_ability + population_size) / 2\nvulnerability_data$exposure <- habitat_specificity\nvulnerability_data$vulnerability <- with(vulnerability_data,\n                                       (sensitivity + adaptive_capacity + exposure) / 3)\n\n# Sort by vulnerability\nvulnerability_data <- vulnerability_data[order(vulnerability_data$vulnerability, decreasing = TRUE), ]\n\n# Visualize vulnerability scores\nggplot(vulnerability_data, aes(x = reorder(species, -vulnerability), y = vulnerability)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(title = \"Climate Change Vulnerability by Species\",\n       x = \"Species\",\n       y = \"Vulnerability Score\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](10-conservation_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=100%}\n:::\n\n```{.r .cell-code}\n\n# Visualize components\nvulnerability_components <- vulnerability_data[, c(\"species\", \"sensitivity\", \"adaptive_capacity\", \"exposure\")]\nvulnerability_long <- vulnerability_components %>% pivot_longer(cols = -species, names_to = \"variable\", values_to = \"value\")\n\nggplot(vulnerability_long, aes(x = reorder(species, -value), y = value, fill = variable)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Components of Climate Change Vulnerability\",\n       x = \"Species\",\n       y = \"Score\",\n       fill = \"Component\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](10-conservation_files/figure-html/unnamed-chunk-11-2.png){fig-align='center' width=100%}\n:::\n:::\n\n\n::: {.callout-note}\n## Code Explanation\n\nThis code demonstrates a trait-based climate change vulnerability assessment:\n\n1. **Assessment Framework**:\n   - Creates a simulated dataset of 12 species with varying traits\n   - Evaluates species on four key traits that affect climate vulnerability:\n     - **Dispersal ability**: Capacity to move to new suitable areas\n     - **Thermal tolerance**: Ability to withstand temperature changes\n     - **Habitat specificity**: Degree of specialization to particular habitats\n     - **Population size**: Indicates demographic resilience\n\n2. **Vulnerability Calculation**:\n   - Transforms trait scores into three vulnerability components:\n     - **Sensitivity**: Physiological tolerance to climate changes\n     - **Adaptive capacity**: Ability to respond through dispersal or adaptation\n     - **Exposure**: Likelihood of experiencing significant change\n   - Combines these components into an overall vulnerability score\n\n3. **Visualization Approach**:\n   - Creates a ranked bar chart of overall vulnerability\n   - Provides a component-wise breakdown to show vulnerability drivers\n   - Uses consistent ordering and color-coding for clarity\n:::\n\n::: {.callout-important}\n## Results Interpretation\n\nThe vulnerability assessment reveals important patterns for climate adaptation planning:\n\n1. **Species Prioritization**:\n   - Some species are clearly more vulnerable than others\n   - The most vulnerable species have high scores across multiple components\n   - These species should be prioritized for conservation action\n\n2. **Vulnerability Drivers**:\n   - Different species are vulnerable for different reasons:\n     - Some species have low adaptive capacity but moderate sensitivity\n     - Others have high sensitivity but better adaptive capacity\n     - Exposure varies across species based on habitat specificity\n   - This indicates the need for tailored conservation strategies\n\n3. **Conservation Implications**:\n   - Highly vulnerable species may require:\n     - Assisted migration to suitable habitat\n     - Ex-situ conservation (e.g., captive breeding)\n     - Special protection of climate refugia\n   - Species with high sensitivity but good adaptive capacity may benefit from connectivity conservation\n:::\n\n::: {.callout-tip}\n## PROFESSIONAL TIP: Climate Vulnerability Assessments\n\nWhen conducting climate vulnerability assessments:\n\n1. **Trait Selection**:\n   - Choose traits with demonstrated links to climate vulnerability\n   - Include both intrinsic (biological) and extrinsic (exposure) factors\n   - Consider different climate change aspects (temperature, precipitation, extreme events)\n   - Use traits that can be measured or estimated with available data\n\n2. **Methodology Considerations**:\n   - Weight components based on their relative importance for the taxa\n   - Include uncertainty measures for each trait assessment\n   - Validate results against observed responses when possible\n   - Consider different climate scenarios to evaluate range of outcomes\n\n3. **Application to Conservation**:\n   - Develop vulnerability-specific adaptation strategies\n   - Identify and protect climate refugia for sensitive species\n   - Design conservation corridors oriented along climate gradients\n   - Monitor highly vulnerable species for early detection of impacts\n:::\n\n## Community-Based Conservation Monitoring\n\nInvolving local communities in conservation monitoring can improve data collection and conservation outcomes.\n\n### Example: Analyzing Community Monitoring Data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Simulate community monitoring data\nset.seed(202)\nn_villages <- 5\nn_months <- 24\n\n# Create variables\nvillage <- rep(paste0(\"Village\", 1:n_villages), each = n_months)\nmonth <- rep(1:n_months, times = n_villages)\nyear <- rep(rep(c(1, 2), each = 12), times = n_villages)\n\n# Generate poaching incidents with seasonal pattern and declining trend\nseason <- sin(month * pi / 6) + 1  # Seasonal pattern\ntrend <- -0.03 * (month - 1)  # Declining trend\nvillage_effect <- rep(rnorm(n_villages, 0, 0.5), each = n_months)\nlambda <- exp(1 + 0.5 * season + trend + village_effect)\npoaching <- rpois(n_villages * n_months, lambda)\n\n# Create a data frame\nmonitoring_data <- data.frame(\n  village = factor(village),\n  month = month,\n  year = factor(year),\n  poaching = poaching\n)\n\n# Visualize the data\nggplot(monitoring_data, aes(x = month, y = poaching, color = village, group = village)) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~year, scales = \"free_x\", labeller = labeller(year = c(\"1\" = \"Year 1\", \"2\" = \"Year 2\"))) +\n  labs(title = \"Poaching Incidents Reported by Community Monitors\",\n       x = \"Month\",\n       y = \"Number of Incidents\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](10-conservation_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=100%}\n:::\n\n```{.r .cell-code}\n\n# Analyze trends\nlibrary(MASS)\ntrend_model <- glm.nb(poaching ~ month + village, data = monitoring_data)\nsummary(trend_model)\n#> \n#> Call:\n#> glm.nb(formula = poaching ~ month + village, data = monitoring_data, \n#>     init.theta = 21.97524464, link = log)\n#> \n#> Coefficients:\n#>                  Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)      1.229650   0.181336   6.781 1.19e-11 ***\n#> month           -0.057015   0.008742  -6.522 6.94e-11 ***\n#> villageVillage2  0.545243   0.201645   2.704 0.006852 ** \n#> villageVillage3  0.558968   0.201193   2.778 0.005465 ** \n#> villageVillage4  0.270966   0.211843   1.279 0.200866    \n#> villageVillage5  0.716424   0.196360   3.649 0.000264 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for Negative Binomial(21.9752) family taken to be 1)\n#> \n#>     Null deviance: 199.13  on 119  degrees of freedom\n#> Residual deviance: 136.01  on 114  degrees of freedom\n#> AIC: 468.75\n#> \n#> Number of Fisher Scoring iterations: 1\n#> \n#> \n#>               Theta:  22.0 \n#>           Std. Err.:  23.9 \n#> \n#>  2 x log-likelihood:  -454.752\n\n# Calculate overall trend\ntrend_coef <- coef(trend_model)[\"month\"]\nmonthly_change <- (exp(trend_coef) - 1) * 100\ncat(\"Monthly change in poaching incidents:\", round(monthly_change, 2), \"%\\n\")\n#> Monthly change in poaching incidents: -5.54 %\n\n# Analyze seasonal patterns\nseason_model <- glm.nb(poaching ~ sin(2 * pi * month / 12) + cos(2 * pi * month / 12) + village,\n                      data = monitoring_data)\nsummary(season_model)\n#> \n#> Call:\n#> glm.nb(formula = poaching ~ sin(2 * pi * month/12) + cos(2 * \n#>     pi * month/12) + village, data = monitoring_data, init.theta = 40.9900692, \n#>     link = log)\n#> \n#> Coefficients:\n#>                        Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)             0.48468    0.15815   3.065 0.002180 ** \n#> sin(2 * pi * month/12)  0.64312    0.08539   7.531 5.03e-14 ***\n#> cos(2 * pi * month/12)  0.08170    0.08155   1.002 0.316459    \n#> villageVillage2         0.55310    0.19722   2.805 0.005039 ** \n#> villageVillage3         0.57202    0.19658   2.910 0.003617 ** \n#> villageVillage4         0.28057    0.20754   1.352 0.176412    \n#> villageVillage5         0.72313    0.19186   3.769 0.000164 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for Negative Binomial(40.9901) family taken to be 1)\n#> \n#>     Null deviance: 209.78  on 119  degrees of freedom\n#> Residual deviance: 129.01  on 113  degrees of freedom\n#> AIC: 457.46\n#> \n#> Number of Fisher Scoring iterations: 1\n#> \n#> \n#>               Theta:  41.0 \n#>           Std. Err.:  70.8 \n#> \n#>  2 x log-likelihood:  -441.462\n\n# Compare models\nanova(trend_model, season_model)\n#> Likelihood ratio tests of Negative Binomial Models\n#> \n#> Response: poaching\n#>                                                       Model    theta Resid. df\n#> 1                                           month + village 21.97524       114\n#> 2 sin(2 * pi * month/12) + cos(2 * pi * month/12) + village 40.99007       113\n#>      2 x log-lik.   Test    df LR stat.      Pr(Chi)\n#> 1       -454.7522                                   \n#> 2       -441.4616 1 vs 2     1  13.2906 0.0002667407\n```\n:::\n\n\n::: {.callout-note}\n## Code Explanation\n\nThis code demonstrates analysis of community-based conservation monitoring data:\n\n1. **Data Simulation**:\n   - Creates a simulated dataset of poaching incidents reported by 5 villages over 24 months\n   - Incorporates three key components of real monitoring data:\n     - Seasonal patterns (using sine functions)\n     - Overall trend (declining poaching incidents)\n     - Village-specific variation (random effects)\n\n2. **Analysis Approach**:\n   - Uses a negative binomial model appropriate for count data\n   - Tests both linear trend and seasonal components\n   - Accounts for different baseline rates across villages\n   - Compares models to determine the best explanation for patterns\n\n3. **Visualization Techniques**:\n   - Time series plots showing raw incident counts\n   - Faceting by year to compare patterns\n   - Color-coding by village to show site-specific variations\n   - Clear marking of temporal patterns\n:::\n\n::: {.callout-important}\n## Results Interpretation\n\nThe community monitoring analysis reveals important patterns for conservation management:\n\n1. **Trend Assessment**:\n   - The model indicates a declining trend in poaching incidents\n   - The calculated monthly change quantifies this decline\n   - Statistical significance helps evaluate whether the trend is reliable\n\n2. **Seasonal Patterns**:\n   - The seasonal model reveals cyclical patterns in poaching\n   - These patterns may correlate with:\n     - Wildlife migration or breeding seasons\n     - Agricultural cycles affecting human behavior\n     - Seasonal changes in patrol effectiveness\n\n3. **Village Differences**:\n   - Different villages show varying baseline levels of poaching\n   - Some villages may have stronger declines than others\n   - This spatial heterogeneity can inform targeted interventions\n\n4. **Conservation Implications**:\n   - Provides evidence for the effectiveness of anti-poaching efforts\n   - Helps identify when and where to focus patrol resources\n   - Demonstrates the value of community participation in monitoring\n:::\n\n::: {.callout-tip}\n## PROFESSIONAL TIP: Community-Based Monitoring\n\nWhen implementing community-based conservation monitoring:\n\n1. **Program Design**:\n   - Develop simple, standardized protocols that community members can follow\n   - Provide adequate training and ongoing support\n   - Use local knowledge to determine what and where to monitor\n   - Combine different data types (quantitative and qualitative)\n\n2. **Data Analysis**:\n   - Account for detection bias in volunteer-collected data\n   - Incorporate uncertainty in both data collection and analysis\n   - Validate with professional monitoring when possible\n   - Consider both spatial and temporal patterns\n\n3. **Program Sustainability**:\n   - Provide tangible benefits to participating communities\n   - Create feedback loops so communities see the impact of their data\n   - Build local capacity for data analysis and interpretation\n   - Develop long-term funding strategies and institutional support\n:::\n\n## Environmental Pollution Analysis\n\nUnderstanding pollution patterns is crucial for environmental conservation. Here we analyze plastic pollution data to identify major contributors and trends.\n\n### Example: Analyzing Plastic Pollution Data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\n# Load the plastic pollution dataset\n# This dataset contains information about plastic waste collected during cleanup events\nplastic_data <- read_csv(\"../data/botany/plant_traits.csv\", show_col_types = FALSE)\n\n# View the structure\ncat(\"Dataset dimensions:\", nrow(plastic_data), \"rows,\", ncol(plastic_data), \"columns\\n\")\n#> Dataset dimensions: 13380 rows, 14 columns\n\n# Summarize total plastic by country\ncountry_totals <- plastic_data %>%\n  group_by(country) %>%\n  summarize(\n    total_plastic = sum(grand_total, na.rm = TRUE),\n    total_events = sum(num_events, na.rm = TRUE),\n    total_volunteers = sum(volunteers, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  arrange(desc(total_plastic)) %>%\n  head(15)\n\n# Visualize top polluting countries\nggplot(country_totals, aes(x = reorder(country, total_plastic), y = total_plastic)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Plastic Pollution by Country\",\n    subtitle = \"Total plastic items collected during cleanup events\",\n    x = \"Country\",\n    y = \"Total Plastic Items\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = scales::comma)\n```\n\n::: {.cell-output-display}\n![](10-conservation_files/figure-html/unnamed-chunk-13-1.png){fig-align='center' width=100%}\n:::\n\n```{.r .cell-code}\n\n# Analyze plastic types\nplastic_types <- plastic_data %>%\n  summarize(\n    HDPE = sum(hdpe, na.rm = TRUE),\n    LDPE = sum(ldpe, na.rm = TRUE),\n    PET = sum(pet, na.rm = TRUE),\n    PP = sum(pp, na.rm = TRUE),\n    PS = sum(ps, na.rm = TRUE),\n    PVC = sum(pvc, na.rm = TRUE),\n    Other = sum(o, na.rm = TRUE)\n  ) %>%\n  pivot_longer(everything(), names_to = \"plastic_type\", values_to = \"count\")\n\n# Visualize plastic types\nggplot(plastic_types, aes(x = reorder(plastic_type, -count), y = count, fill = plastic_type)) +\n  geom_col() +\n  labs(\n    title = \"Distribution of Plastic Types in Pollution\",\n    subtitle = \"PET (bottles) is the most common pollutant\",\n    x = \"Plastic Type\",\n    y = \"Total Count\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  scale_y_continuous(labels = scales::comma)\n```\n\n::: {.cell-output-display}\n![](10-conservation_files/figure-html/unnamed-chunk-13-2.png){fig-align='center' width=100%}\n:::\n\n```{.r .cell-code}\n\n# Top corporate contributors\ntop_companies <- plastic_data %>%\n  filter(parent_company != \"Grand Total\", parent_company != \"Unbranded\") %>%\n  group_by(parent_company) %>%\n  summarize(total = sum(grand_total, na.rm = TRUE), .groups = \"drop\") %>%\n  arrange(desc(total)) %>%\n  head(10)\n\nggplot(top_companies, aes(x = reorder(parent_company, total), y = total)) +\n  geom_col(fill = \"coral\") +\n  coord_flip() +\n  labs(\n    title = \"Top Corporate Contributors to Plastic Pollution\",\n    subtitle = \"Based on branded plastic items collected\",\n    x = \"Company\",\n    y = \"Total Plastic Items\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = scales::comma)\n```\n\n::: {.cell-output-display}\n![](10-conservation_files/figure-html/unnamed-chunk-13-3.png){fig-align='center' width=100%}\n:::\n:::\n\n\n::: {.callout-important}\n## Conservation Implications\n\nPlastic pollution analysis reveals important patterns for environmental management:\n\n1. **Geographic Hotspots**: Identifying countries with highest pollution levels helps target intervention efforts\n2. **Plastic Types**: Understanding which plastics dominate (e.g., PET bottles) informs recycling and reduction strategies\n3. **Corporate Responsibility**: Tracking branded items helps hold companies accountable for their environmental impact\n4. **Volunteer Engagement**: Cleanup events provide both data and community engagement opportunities\n:::\n\n## Extreme Weather and Climate Events\n\nClimate change is increasing the frequency and intensity of extreme weather events, which have significant impacts on ecosystems and conservation efforts.\n\n### Example: Analyzing Storm Data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load the storm/hurricane dataset\nstorms <- read_csv(\"../data/epidemiology/disease_data.csv\", show_col_types = FALSE)\n\n# View structure\ncat(\"Storm records:\", nrow(storms), \"\\n\")\n#> Storm records: 19537\ncat(\"Years covered:\", min(storms$year), \"-\", max(storms$year), \"\\n\")\n#> Years covered: 1975 - 2022\n\n# Summarize storms by year\nannual_storms <- storms %>%\n  group_by(year) %>%\n  summarize(\n    n_storms = n_distinct(name),\n    max_wind = max(wind, na.rm = TRUE),\n    avg_pressure = mean(pressure, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\n# Visualize storm frequency over time\nggplot(annual_storms, aes(x = year, y = n_storms)) +\n  geom_line(color = \"darkblue\", linewidth = 1) +\n  geom_smooth(method = \"loess\", color = \"red\", se = TRUE, alpha = 0.2) +\n  labs(\n    title = \"Annual Storm Frequency (1975-2020)\",\n    subtitle = \"Trend line shows potential increase in storm activity\",\n    x = \"Year\",\n    y = \"Number of Named Storms\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](10-conservation_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=100%}\n:::\n\n```{.r .cell-code}\n\n# Analyze storm intensity by category\ncategory_summary <- storms %>%\n  filter(!is.na(category)) %>%\n  group_by(category) %>%\n  summarize(\n    count = n(),\n    avg_wind = mean(wind, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\nggplot(category_summary, aes(x = factor(category), y = count, fill = factor(category))) +\n  geom_col() +\n  scale_fill_viridis_d(option = \"plasma\") +\n  labs(\n    title = \"Storm Observations by Hurricane Category\",\n    x = \"Hurricane Category\",\n    y = \"Number of Observations\",\n    fill = \"Category\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](10-conservation_files/figure-html/unnamed-chunk-14-2.png){fig-align='center' width=100%}\n:::\n\n```{.r .cell-code}\n\n# Spatial distribution of storms\nggplot(storms %>% filter(!is.na(lat), !is.na(long)), \n       aes(x = long, y = lat, color = wind)) +\n  geom_point(alpha = 0.3, size = 0.5) +\n  scale_color_viridis_c(option = \"inferno\") +\n  labs(\n    title = \"Spatial Distribution of Storm Activity\",\n    subtitle = \"Color indicates wind speed (knots)\",\n    x = \"Longitude\",\n    y = \"Latitude\",\n    color = \"Wind Speed\"\n  ) +\n  theme_minimal() +\n  coord_fixed(ratio = 1.3)\n```\n\n::: {.cell-output-display}\n![](10-conservation_files/figure-html/unnamed-chunk-14-3.png){fig-align='center' width=100%}\n:::\n\n```{.r .cell-code}\n\n# Test for trend in maximum wind speeds\nwind_trend <- lm(max_wind ~ year, data = annual_storms)\nsummary(wind_trend)\n#> \n#> Call:\n#> lm(formula = max_wind ~ year, data = annual_storms)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -51.856 -12.068  -0.065  15.186  42.583 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)\n#> (Intercept) -443.9425   425.9010  -1.042    0.303\n#> year           0.2860     0.2131   1.342    0.186\n#> \n#> Residual standard error: 20.45 on 46 degrees of freedom\n#> Multiple R-squared:  0.03769,\tAdjusted R-squared:  0.01677 \n#> F-statistic: 1.802 on 1 and 46 DF,  p-value: 0.1861\n```\n:::\n\n\n::: {.callout-note}\n## Ecological Relevance\n\nExtreme weather events have profound impacts on ecosystems:\n\n1. **Habitat Destruction**: Hurricanes can devastate coastal habitats, coral reefs, and forests\n2. **Species Displacement**: Storm surges and flooding force wildlife to relocate\n3. **Ecosystem Recovery**: Understanding storm patterns helps predict recovery timelines\n4. **Conservation Planning**: Storm frequency data informs the design of resilient protected areas\n5. **Climate Adaptation**: Trends in storm intensity guide climate adaptation strategies\n:::\n\n## Summary\n\nIn this chapter, we've explored how data analysis techniques can be applied to conservation challenges:\n\n- Species distribution modeling to predict habitat suitability\n- Population trend analysis to monitor species status\n- Habitat fragmentation analysis to assess landscape connectivity\n- Protected area effectiveness evaluation using BACI designs\n- Threat assessment and prioritization for conservation planning\n- Systematic conservation planning using complementarity analysis\n- Climate change vulnerability assessment based on species traits\n- Community-based conservation monitoring to track threats\n- Environmental pollution analysis to identify hotspots and corporate contributors\n- Extreme weather event analysis to understand climate impacts on ecosystems\n\nThese applications demonstrate how the statistical methods covered throughout this book can help address real-world conservation problems, inform management decisions, and ultimately contribute to biodiversity conservation.\n\n## Exercises\n\n1. Import a dataset on species occurrences and environmental variables, then build a simple species distribution model.\n2. Analyze population monitoring data to detect trends and assess conservation status.\n3. Calculate basic landscape metrics for a land cover map to quantify habitat fragmentation.\n4. Design and analyze a BACI study to evaluate the effectiveness of a conservation intervention.\n5. Conduct a threat assessment for a species or ecosystem of your choice.\n6. Use complementarity analysis to identify priority sites for conservation.\n7. Perform a climate change vulnerability assessment for a group of species.\n8. Analyze community monitoring data to detect trends in threats or biodiversity.\n\n## Chapter Summary\n\n### Key Concepts\n\n-   **Biodiversity Assessment**: Quantifying species richness and diversity is fundamental for conservation planning\n-   **Population Monitoring**: Tracking abundance trends over time helps identify species at risk\n-   **Impact Evaluation**: Rigorous statistical designs (like BACI) are needed to assess conservation effectiveness\n-   **Threat Prioritization**: Data-driven methods help allocate limited resources to the most pressing threats\n-   **Adaptive Management**: Using data analysis to iteratively improve conservation strategies\n\n### R Functions Applied\n\n-   `diversity()` (vegan package) - Calculate diversity indices\n-   `specaccum()` (vegan package) - Species accumulation curves\n-   `glm()` - Generalized linear models for population trends\n-   `lme()` (nlme package) - Linear mixed-effects models for hierarchical data\n-   `ggplot()` - Visualizing trends and comparisons\n\n### Next Steps\n\nThis concludes the main content of the book. The final chapter, **Advanced Modeling Techniques**, provides a glimpse into more complex analytical methods for those wishing to take their skills further.\n\n## Exercises\n\n1.  **Biodiversity**: Calculate Shannon and Simpson diversity indices for a community dataset of your choice. Compare the results.\n2.  **Population Trend**: Fit a linear and a non-linear model to a population time series. Which model fits better?\n3.  **Intervention Analysis**: Design a theoretical study to evaluate the impact of a new protected area. What data would you collect?\n4.  **Threat Assessment**: Create a simple threat matrix for a local ecosystem, scoring threats based on severity and scope.\n5.  **Communication**: Write a one-page policy brief summarizing the results of a conservation analysis for a non-technical audience.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}