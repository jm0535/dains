---
prefer-html: true
---

# Introduction to Data Analysis

## Overview

Data analysis is a critical skill in modern natural sciences research [@wickham2016r; @zuur2009mixed]. This chapter introduces the fundamental concepts, tools, and approaches that form the foundation of effective data analysis across various scientific disciplines, with a focus on modern tidyverse and tidymodels frameworks.

## Why Data Analysis Matters in Natural Sciences

Data analysis plays a pivotal role in natural sciences research for several reasons:

1. **Evidence-Based Decision Making**: Data analysis transforms raw observations into actionable insights, enabling researchers and practitioners to make informed decisions about conservation strategies, resource management practices, agricultural planning, environmental interventions, and more [@bolker2009generalized].

2. **Pattern Recognition**: Through statistical analysis, researchers can identify patterns, trends, and relationships within natural systems that might not be apparent from casual observation alone [@zuur2007analyzing]. This applies to diverse fields including ecology, geology, marine biology, atmospheric science, and agriculture.

3. **Hypothesis Testing**: Data analysis provides rigorous methods to test hypotheses about natural phenomena, allowing researchers to build and refine scientific theories about how natural systems function [@gotelli2004null]. This is fundamental across all scientific disciplines.

4. **Prediction and Modeling**: Advanced analytical techniques enable the development of predictive models that can forecast changes in natural systems, such as species distribution shifts under climate change, crop yield predictions, geological processes, weather patterns, and more [@elith2009species].

::: {.callout-tip}
## PROFESSIONAL TIP: Principles of Robust Experimental Design

Before diving into data analysis, ensure your experimental design follows these key principles:

- **Formulate clear hypotheses**: Define specific, testable hypotheses before collecting data
- **Control for confounding variables**: Identify and account for factors that might influence your results
- **Randomize appropriately**: Randomly assign treatments to experimental units to reduce bias
- **Include adequate replication**: Ensure sufficient sample sizes for statistical power (use power analysis)
- **Consider spatial and temporal scales**: Match your sampling design to the scales of the processes being studied
- **Plan for appropriate controls**: Include positive, negative, and procedural controls as needed
- **Use factorial designs when appropriate**: Efficiently test multiple factors and their interactions
- **Consider blocking**: Group experimental units to account for known sources of variation
- **Pre-register your study**: Document your hypotheses and analysis plan before collecting data
- **Plan for appropriate statistical analysis**: Select statistical methods based on your design, not just your results
:::

## Tools for Data Analysis

This book focuses on R and RStudio as the primary tools for data analysis, using the tidyverse and tidymodels frameworks:

### R and RStudio

R is a powerful programming language and environment specifically designed for statistical computing and graphics. RStudio is an integrated development environment (IDE) that makes working with R more accessible and efficient.

Key advantages of R include:

- **Open-source and free**: Available to anyone without cost
- **Extensive package ecosystem**: Thousands of specialized packages for various types of analyses across all scientific disciplines
- **Reproducibility**: Code-based approach ensures analyses can be repeated and verified
- **Flexibility**: Can be adapted to virtually any analytical need in the natural sciences
- **Active community**: Large user base provides support and continuous development
- **Modern frameworks**: Tidyverse and tidymodels provide consistent, intuitive workflows

### The Tidyverse Philosophy

The tidyverse is a collection of R packages that share a common design philosophy, grammar, and data structures. It makes data analysis more intuitive, readable, and reproducible.

**Core Principles**:

1. **Tidy Data**: Each variable forms a column, each observation forms a row
2. **Pipe Workflow**: Chain operations together with `%>%` (or `|>`)
3. **Consistent API**: Functions work together seamlessly
4. **Human-Readable Code**: Code reads like natural language

**Core Tidyverse Packages**:
- `readr`: Fast, friendly data import
- `dplyr`: Data manipulation (filter, select, mutate, summarize)
- `ggplot2`: Data visualization using grammar of graphics
- `tidyr`: Data tidying (pivot, separate, unite)
- `purrr`: Functional programming tools
- `tibble`: Modern data frames with better defaults
- `stringr`: String manipulation
- `forcats`: Factor (categorical variable) tools

**Example Tidyverse Workflow**:

```{r}
#| label: tidyverse-example
#| echo: true
#| message: false
#| warning: false

# Load the tidyverse packages
library(tidyverse)

# Load the Palmer penguins dataset
penguins <- read_csv("../data/environmental/climate_data.csv",
                     show_col_types = FALSE)

# Modern tidyverse workflow with pipes
penguins %>%
  filter(!is.na(bill_length_mm)) %>%      # Remove missing values
  group_by(species) %>%                    # Group by species
  summarize(                               # Calculate summaries
    mean_bill = mean(bill_length_mm),
    sd_bill = sd(bill_length_mm),
    n = n()
  ) %>%
  arrange(desc(mean_bill))                 # Sort by mean bill length
```

::: {.callout-note}
## Code Explanation

This code demonstrates the modern tidyverse workflow:

1. **`library(tidyverse)`**: Loads all core tidyverse packages at once
2. **`read_csv()`**: Fast CSV import with automatic type detection (better than base R's `read.csv()`)
3. **`%>%` (pipe operator)**: Chains operations together, reading left-to-right
4. **`filter()`**: Keeps only rows where bill_length_mm is not missing
5. **`group_by()`**: Groups data by species for grouped operations
6. **`summarize()`**: Calculates summary statistics for each group
7. **`n()`**: Counts the number of observations in each group
8. **`arrange()`**: Sorts results in descending order by mean bill length

**Key Advantages**:
- Code reads like a recipe: "Take penguins, then filter, then group, then summarize"
- Each step is clear and explicit
- Easy to modify or extend
- No intermediate variables needed
:::

::: {.callout-important}
## Results Interpretation

The output shows penguin bill length statistics by species:

- **Gentoo penguins** have the longest average bill length (~47 mm)
- **Chinstrap penguins** have intermediate bill length (~48-49 mm)
- **Adelie penguins** have the shortest average bill length (~39 mm)
- **Standard deviations** indicate variation within each species
- **Sample sizes** (n) show we have good representation of each species

This table format is much cleaner than base R output and ready for publication!
:::

### Visualizing Data with ggplot2

Data visualization is crucial for understanding patterns. The `ggplot2` package uses the "grammar of graphics" for creating publication-quality plots.

```{r}
#| label: penguin-visualization
#| fig-cap: "Distribution of penguin bill lengths by species"
#| fig-width: 8
#| fig-height: 5
#| warning: false

penguins %>%
  filter(!is.na(bill_length_mm), !is.na(species)) %>%
  ggplot(aes(x = species, y = bill_length_mm, fill = species)) +
  geom_boxplot(alpha = 0.7, outlier.shape = NA) +
  geom_jitter(width = 0.2, alpha = 0.3, size = 2) +
  labs(
    title = "Penguin Bill Length by Species",
    subtitle = "Palmer Archipelago, Antarctica",
    x = "Species",
    y = "Bill Length (mm)",
    caption = "Data: Palmer Station LTER"
  ) +
  theme_minimal(base_size = 12) +
  theme(legend.position = "none") +
  scale_fill_viridis_d(option = "plasma", begin = 0.2, end = 0.8)
```

::: {.callout-note}
## Code Explanation

This visualization demonstrates ggplot2's layered grammar of graphics:

1. **Data Pipeline**: Starts with data, filters missing values
2. **`ggplot(aes(...))`**: Sets up the plot with aesthetic mappings
   - `x = species`: Species on x-axis
   - `y = bill_length_mm`: Bill length on y-axis
   - `fill = species`: Color boxes by species
3. **`geom_boxplot()`**: Adds box-and-whisker plots showing distribution
4. **`geom_jitter()`**: Adds individual data points with random jitter
5. **`labs()`**: Provides informative labels and title
6. **`theme_minimal()`**: Applies clean, publication-ready theme
7. **`scale_fill_viridis_d()`**: Uses colorblind-friendly palette

**Why This Matters**:
- Layered approach is intuitive and flexible
- Easy to modify individual components
- Automatically handles legends and scales
- Publication-quality output by default
:::

::: {.callout-important}
## Results Interpretation

The visualization reveals several insights:

1. **Clear Species Differences**: Gentoo and Chinstrap have notably longer bills than Adelie
2. **Variation Patterns**: Each species shows different levels of within-group variation
3. **Distribution Shape**: Box plots show median (center line), quartiles (box), and range (whiskers)
4. **Individual Observations**: Jittered points reveal sample sizes and outliers
5. **No Obvious Outliers**: Data appears normally distributed within each species

**Visual vs. Numerical**: This single plot communicates more effectively than tables of statistics. It's easier to see patterns, compare groups, and identify unusual observations.
:::

::: {.callout-tip}
## PROFESSIONAL TIP: Why Tidyverse for Natural Sciences?

The tidyverse approach offers specific advantages for natural sciences research:

1. **Reproducibility**: Clear, readable code is easier to verify and share with reviewers
2. **Collaboration**: Standardized syntax helps research teams work together effectively
3. **Learning Curve**: Consistent patterns across packages reduce cognitive load
4. **Documentation**: Excellent resources (R for Data Science, tidyverse.org)
5. **Community**: Large, active user base in ecology, evolution, and environmental science
6. **Integration**: Works seamlessly with RStudio, Quarto, and modern reporting tools
7. **Publication**: Tidy outputs integrate easily with journal requirements
8. **Teaching**: Easier for students and collaborators to learn than base R

**Real-World Impact**: Many leading ecology and environmental science labs have adopted tidyverse as their standard workflow, making it essential for collaboration and career development.
:::

## Setting Up Your Environment

### Installing R and RStudio

To install R and RStudio:

1. Download and install R from [CRAN](https://cran.r-project.org/)
2. Download and install RStudio from [Posit](https://posit.co/downloads/)

### Essential R Packages

For the analyses in this book, you'll need tidyverse and tidymodels ecosystems. Install them with:

```{r}
#| label: install-packages
#| eval: false

# Install tidyverse and tidymodels ecosystems
install.packages(c(
  # Core tidyverse (data manipulation and visualization)
  "tidyverse",     # Meta-package: dplyr, ggplot2, tidyr, readr, purrr, tibble, stringr, forcats

  # Tidymodels (statistical modeling and machine learning)
  "tidymodels",    # Meta-package: parsnip, recipes, workflows, tune, yardstick, rsample

  # Data exploration and summary
  "skimr",         # Enhanced data summaries with histograms
  "naniar",        # Missing data visualization and analysis
  "visdat",        # Data structure visualization

  # Statistical analysis (tidy-friendly)
  "rstatix",       # Tidy statistical tests
  "infer",         # Tidy statistical inference
  "broom",         # Tidy model outputs

  # Visualization enhancements
  "patchwork",     # Combine multiple plots
  "ggridges",      # Ridge plots for distributions
  "ggdist",        # Distribution visualizations
  "viridis",       # Colorblind-friendly palettes

  # Document generation
  "knitr",         # Dynamic documents
  "rmarkdown",     # R Markdown documents
  "gt",            # Publication-quality tables
  "gtsummary"      # Summary tables for regression models
))
```

::: {.callout-tip}
## PROFESSIONAL TIP: Package Management

For reproducible research, consider using `renv` to lock package versions:

```r
# Initialize renv in your project
renv::init()

# After installing packages, save the state
renv::snapshot()

# Restore packages on a new machine
renv::restore()
```

This ensures collaborators and reviewers use identical package versions.
:::

## The Modern Data Analysis Workflow

Effective data analysis follows a structured, reproducible workflow. We use the tidyverse and tidymodels frameworks throughout this book.

**Workflow Stages**:

1. **Import** (`readr`, `haven`, `readxl`)
   - Load data from CSV, Excel, SPSS, Stata, etc.
   - Handle different file formats consistently
   - Specify column types explicitly

2. **Tidy** (`tidyr`)
   - Reshape data to tidy format (one observation per row)
   - Handle missing values systematically
   - Separate or unite columns as needed

3. **Transform** (`dplyr`)
   - Filter rows based on conditions
   - Select relevant columns
   - Create new variables with `mutate()`
   - Summarize data with `summarize()`
   - Group operations with `group_by()`

4. **Visualize** (`ggplot2`)
   - Explore patterns and relationships
   - Identify outliers and anomalies
   - Communicate findings effectively
   - Create publication-quality figures

5. **Model** (`tidymodels`)
   - Specify statistical models
   - Fit models to data
   - Validate model performance
   - Make predictions

6. **Communicate** (`Quarto`, `rmarkdown`)
   - Create reproducible reports
   - Generate figures and tables
   - Share findings with collaborators
   - Publish to journals or web

**Complete Workflow Example**:

```{r}
#| label: complete-workflow
#| message: false
#| fig-width: 8
#| fig-height: 5

library(tidyverse)
library(tidymodels)

# 1. Import
penguins_raw <- read_csv("../data/environmental/climate_data.csv",
                         show_col_types = FALSE)

# 2-3. Tidy and Transform
penguins_clean <- penguins_raw %>%
  # Remove missing values
  filter(!is.na(bill_length_mm),
         !is.na(bill_depth_mm),
         !is.na(flipper_length_mm),
         !is.na(body_mass_g)) %>%
  # Select relevant columns
  select(species, island, sex,
         bill_length_mm, bill_depth_mm,
         flipper_length_mm, body_mass_g) %>%
  # Create new variable (bill ratio)
  mutate(bill_ratio = bill_length_mm / bill_depth_mm)

# 4. Visualize
penguins_clean %>%
  ggplot(aes(x = flipper_length_mm, y = body_mass_g, color = species)) +
  geom_point(alpha = 0.6, size = 2) +
  geom_smooth(method = "lm", se = FALSE, linewidth = 1) +
  labs(
    title = "Penguin Body Mass vs. Flipper Length",
    x = "Flipper Length (mm)",
    y = "Body Mass (g)",
    color = "Species"
  ) +
  theme_minimal() +
  scale_color_viridis_d(option = "plasma", begin = 0.2, end = 0.8)

# 5. Model (simple linear model example)
penguin_model <- linear_reg() %>%
  set_engine("lm") %>%
  fit(body_mass_g ~ flipper_length_mm + species, data = penguins_clean)

# Tidy model output
tidy(penguin_model)

# 6. Communicate (this document is created with Quarto!)
```

::: {.callout-tip}
## PROFESSIONAL TIP: Reproducible Workflows

For natural sciences research, maintain reproducibility by:

1. **Use RStudio Projects**: Keep all files together, use relative paths
2. **Never Use Absolute Paths**: `"C:/Users/..."` breaks on other computers
3. **Version Control**: Use Git to track all code changes
4. **Document Everything**: Comment your code, use R Markdown/Quarto
5. **Session Info**: Always record R and package versions
6. **Set Seeds**: Use `set.seed()` for reproducible random processes
7. **Package Management**: Use `renv` to lock package versions
8. **Data Provenance**: Document data sources and processing steps

```{r}
#| label: session-info
# Record your computational environment
sessionInfo()
```

**Why This Matters**: Reviewers and readers can reproduce your exact analysis, increasing trust and enabling verification of your findings.
:::

## Introduction to Tidymodels

While we'll cover statistical modeling in detail in later chapters, it's worth introducing the tidymodels framework early. Tidymodels provides a consistent, tidy interface for statistical and machine learning models.

**Why Tidymodels?**

Traditional R modeling uses different syntax for different model types:
- `lm()` for linear models
- `glm()` for generalized linear models
- `randomForest()` for random forests
- Each has different syntax and output formats

Tidymodels provides a **unified interface** for all models:

```{r}
#| label: tidymodels-intro
#| message: false

library(tidymodels)

# Step 1: Specify the model type
model_spec <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

# Step 2: Fit the model to data
model_fit <- model_spec %>%
  fit(bill_length_mm ~ bill_depth_mm + flipper_length_mm,
      data = penguins_clean)

# Step 3: Get tidy results
tidy(model_fit)

# Step 4: Model performance metrics
glance(model_fit)

# Step 5: Make predictions
predictions <- predict(model_fit, new_data = penguins_clean)
head(predictions)
```

**Core Tidymodels Packages**:

- **parsnip**: Unified model specification (linear models, random forests, neural networks, etc.)
- **recipes**: Feature engineering and preprocessing pipelines
- **workflows**: Combine preprocessing and modeling steps
- **tune**: Hyperparameter tuning with cross-validation
- **yardstick**: Model performance metrics
- **rsample**: Data splitting and resampling
- **broom**: Tidy model outputs (tidy, glance, augment)

**Benefits for Natural Sciences**:

1. **Consistency**: Same syntax whether you're fitting linear models or machine learning algorithms
2. **Best Practices**: Built-in cross-validation, preprocessing, and metrics
3. **Reproducibility**: Workflows are explicit and documentable
4. **Modern Methods**: Easy access to cutting-edge algorithms
5. **Integration**: Works seamlessly with tidyverse
6. **Publication**: Tidy outputs integrate with reporting tools

::: {.callout-note}
## Looking Ahead

We'll use tidymodels throughout this book for:
- Chapter 4-5: Hypothesis testing and statistical tests
- Chapter 8: Regression analysis
- Chapter 9: Predictive modeling for conservation

The consistent interface means you'll learn one syntax that works for all analyses!
:::

## Tidy Data Principles

Regardless of data type, organizing data in a "tidy" format makes analysis easier and more efficient.

**Tidy Data Rules** [@wickham2014tidy]:

1. **Each variable forms a column**
2. **Each observation forms a row**
3. **Each type of observational unit forms a table**

**Example: Untidy vs. Tidy Data**

**Untidy Data** (wide format - species as columns):

```{r}
#| label: untidy-example
# Untidy: Multiple variables in column names
untidy_data <- tibble(
  site = c("Forest A", "Forest B", "Forest C"),
  species_oak_count = c(10, 15, 12),
  species_pine_count = c(8, 11, 9),
  species_birch_count = c(5, 7, 6)
)

untidy_data
```

**Tidy Data** (long format - one observation per row):

```{r}
#| label: tidy-example
# Tidy: One observation per row
tidy_data <- untidy_data %>%
  pivot_longer(
    cols = starts_with("species"),
    names_to = "species",
    values_to = "count",
    names_prefix = "species_",
    names_pattern = "(.*)_count"
  )

tidy_data
```

**Why Tidy Data Matters**:

- ✅ Works seamlessly with `dplyr` verbs (filter, mutate, summarize)
- ✅ Easy to visualize with `ggplot2`
- ✅ Simpler to perform grouped operations
- ✅ Better for statistical modeling
- ✅ More flexible for different analyses
- ✅ Easier to join with other datasets

**Common Tidying Operations**:

```{r}
#| label: tidying-operations
#| eval: false

# Wide to long (pivot_longer)
data %>% pivot_longer(cols = c(var1, var2, var3), names_to = "variable", values_to = "value")

# Long to wide (pivot_wider)
data %>% pivot_wider(names_from = variable, values_from = value)

# Separate one column into multiple
data %>% separate(col, into = c("var1", "var2"), sep = "_")

# Unite multiple columns into one
data %>% unite(col = "new_col", c(var1, var2), sep = "_")
```

::: {.callout-tip}
## PROFESSIONAL TIP: When to Use Wide vs. Long Format

**Use Long Format (Tidy) For**:
- Statistical analysis (regression, ANOVA)
- Creating plots with ggplot2
- Grouped operations (by species, site, treatment)
- Most tidyverse functions

**Use Wide Format For**:
- Data entry (easier to input)
- Reporting tables (easier to read)
- Some specific analyses (correlation matrices)

**Best Practice**: Keep raw data in the format it was collected, but **transform to tidy/long format for analysis**.
:::

## Types of Data in Natural Sciences Research

Research across the natural sciences involves several types of data:

### Categorical Data

Categorical data represent qualitative characteristics, such as:
- Species names or taxonomic classifications
- Habitat or ecosystem types
- Rock or soil classifications
- Land-use categories
- Treatment groups in experiments
- Genetic markers

**Tidyverse Tools**: `forcats` package for factor manipulation

```{r}
#| label: categorical-example
#| eval: false

# Reorder factor levels by frequency
penguins %>%
  mutate(species = fct_infreq(species))

# Lump infrequent levels together
penguins %>%
  mutate(island = fct_lump(island, n = 2))
```

### Numerical Data

Numerical data involve measurements or counts:
- Continuous measurements (e.g., temperature, pH, concentration, biomass)
- Discrete counts (e.g., number of individuals, species richness)
- Rates (e.g., growth rates, reaction rates, decomposition rates)
- Ratios and indices (e.g., diversity indices, chemical ratios)

**Tidyverse Tools**: `dplyr` for transformations, `ggplot2` for visualization

### Spatial Data

Spatial data describe geographical distributions:
- Coordinates (latitude/longitude)
- Elevation or depth
- Topographic features
- Land cover maps
- Remote sensing data

**Tidyverse Tools**: `sf` package integrates with tidyverse for spatial analysis

### Temporal Data

Temporal data track changes over time:
- Time series of measurements
- Seasonal patterns
- Long-term monitoring data
- Growth curves

**Tidyverse Tools**: `lubridate` for dates/times, `tsibble` for time series

Understanding the type of data you're working with is crucial for selecting appropriate analytical methods across all natural science disciplines.

## Try It Yourself: Interactive Exploration

Now practice with these exercises using the tidyverse workflow:

**Exercise 1: Filter and Summarize**

```{r}
#| label: exercise-1-solution
#| eval: false

# Find the average body mass for each species on each island
penguins_clean %>%
  filter(!is.na(body_mass_g)) %>%
  group_by(species, island) %>%
  summarize(
    avg_mass = mean(body_mass_g),
    sd_mass = sd(body_mass_g),
    n_penguins = n(),
    .groups = "drop"
  ) %>%
  arrange(species, desc(avg_mass))
```

**Exercise 2: Create a Visualization**

```{r}
#| label: exercise-2-solution
#| eval: false

# Compare bill dimensions between male and female penguins
penguins_clean %>%
  filter(!is.na(sex)) %>%
  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = sex)) +
  geom_point(alpha = 0.6) +
  facet_wrap(~species) +
  labs(
    title = "Penguin Bill Dimensions by Sex and Species",
    x = "Bill Length (mm)",
    y = "Bill Depth (mm)",
    color = "Sex"
  ) +
  theme_minimal() +
  scale_color_viridis_d(option = "plasma", begin = 0.3, end = 0.7)
```

**Exercise 3: Data Transformation**

```{r}
#| label: exercise-3-solution
#| eval: false

# Create a new variable and compare groups
penguins_clean %>%
  mutate(size_category = case_when(
    body_mass_g < 3500 ~ "Small",
    body_mass_g < 4500 ~ "Medium",
    TRUE ~ "Large"
  )) %>%
  count(species, size_category) %>%
  pivot_wider(names_from = size_category, values_from = n, values_fill = 0)
```

::: {.callout-note}
## Learning by Doing

The best way to learn tidyverse is through practice. As you work through these exercises:

1. **Experiment**: Modify the code and observe the results
2. **Break Things**: Errors are valuable learning opportunities
3. **Read Help**: Use `?function_name` to learn more details
4. **Explore**: Try different variables and groupings
5. **Visualize**: Always examine your data graphically
6. **Share**: Discuss findings with colleagues
:::

## Summary

In this chapter, we've introduced:

1. **Why data analysis matters** in natural sciences research
2. **Modern tools**: R, RStudio, tidyverse, and tidymodels
3. **Tidyverse philosophy**: Tidy data, pipes, consistent API
4. **Data visualization**: ggplot2 for publication-quality graphics
5. **Complete workflow**: Import → Tidy → Transform → Visualize → Model → Communicate
6. **Tidymodels preview**: Unified interface for statistical modeling
7. **Tidy data principles**: Long vs. wide format
8. **Data types**: Categorical, numerical, spatial, temporal

**Key Takeaways**:
- Tidyverse makes code more readable and reproducible
- ggplot2 creates publication-quality visualizations
- Tidymodels provides consistent modeling interface
- Tidy data format works best with these tools
- Modern workflow emphasizes reproducibility

In the next chapter, we'll dive deeper into data basics, learning how to import, clean, and prepare data for analysis using tidyverse tools.

## Exercises

1. **Install and Load**:
   - Install R and RStudio
   - Install tidyverse and tidymodels packages
   - Load tidyverse with `library(tidyverse)`

2. **Explore Built-in Data**:
   ```r
   # Use tidyverse functions on built-in datasets
   glimpse(iris)
   iris %>% count(Species)
   iris %>% ggplot(aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
     geom_point()
   ```

3. **Practice Pipes**:
   - Take the `mtcars` dataset
   - Filter for cars with mpg > 20
   - Select only mpg, hp, and wt columns
   - Arrange by hp in descending order
   - Use pipes (`%>%`) to chain these operations

4. **Create Your First Visualization**:
   - Use the `penguins` data from this chapter
   - Create a scatter plot of two variables of your choice
   - Add appropriate labels and a theme
   - Use a colorblind-friendly palette

5. **Think About Your Research**:
   - What research question interests you in your field?
   - What type of data would you need?
   - Is it currently in tidy format?
   - How would you visualize it?

6. **Explore Documentation**:
   - Visit [tidyverse.org](https://www.tidyverse.org/)
   - Browse the [R for Data Science](https://r4ds.hadley.nz/) book
   - Try the examples from the dplyr or ggplot2 cheat sheets
