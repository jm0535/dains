---
prefer-html: true
---

# Regression Analysis

## Introduction

Regression analysis is a powerful statistical tool for modeling relationships between variables. This chapter explores different types of regression models and their applications in natural sciences research.

## Linear Regression

Linear regression models the relationship between a dependent variable and one or more independent variables:

```{r}
# Load required packages
library(tidyverse)      # For data manipulation
library(ggplot2)        # For visualization
library(broom)          # For tidying model outputs
library(performance)    # For model diagnostics
library(see)            # For visualization of model diagnostics
library(parameters)     # For parameter description
library(ggeffects)      # For visualizing model effects

# Set a professional theme for all plots
theme_set(theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 12, color = "gray40"),
    axis.title = element_text(face = "bold"),
    legend.position = "bottom",
    panel.grid.minor = element_blank(),
    panel.border = element_rect(color = "gray90", fill = NA, linewidth = 0.5)
  ))

# Load the Palmer penguins dataset (stored as climate_data.csv)
penguins <- read_csv("../data/environmental/climate_data.csv", show_col_types = FALSE)

# Remove rows with missing values in the key variables we'll use for regression
penguins_clean <- penguins %>%
  filter(!is.na(bill_length_mm), !is.na(body_mass_g),
         !is.na(bill_depth_mm), !is.na(flipper_length_mm)) %>%
  # Add species as a factor for proper modeling
  mutate(species = as.factor(species))

# Create a linear regression model
model <- lm(body_mass_g ~ bill_length_mm, data = penguins_clean)

# Get model summary with broom for cleaner output
model_summary <- summary(model)
model_tidy <- tidy(model, conf.int = TRUE)
model_glance <- glance(model)

# Display key model metrics
cat("Model Summary:\n")
cat(paste0("R² = ", round(model_glance$r.squared, 3),
          ", Adjusted R² = ", round(model_glance$adj.r.squared, 3), "\n"))
cat(paste0("F-statistic: ", round(model_glance$statistic, 2),
          " on ", model_glance$df, " and ", model_glance$df.residual,
          " DF, p-value: ", format.pval(model_glance$p.value, digits = 3), "\n\n"))

# Create a more professional table of coefficients
library(gt)
model_tidy %>%
  gt() %>%
  tab_header(title = "Linear Regression Coefficients") %>%
  fmt_number(columns = c("estimate", "std.error", "statistic", "p.value", "conf.low", "conf.high"), decimals = 3)

# Create an enhanced scatter plot with regression line
ggplot(penguins_clean, aes(x = bill_length_mm, y = body_mass_g)) +
  # Add data points with some transparency
  geom_point(aes(color = species), alpha = 0.7, size = 3) +
  # Add regression line with confidence interval
  geom_smooth(method = "lm", color = "darkred", fill = "pink", alpha = 0.2) +
  # Add annotations for R² and p-value
  annotate("text", x = min(penguins_clean$bill_length_mm) + 1,
           y = max(penguins_clean$body_mass_g) - 500,
           label = paste0("R² = ", round(model_glance$r.squared, 3),
                         "\np < ", format.pval(model_glance$p.value, digits = 3)),
           hjust = 0, size = 4, color = "darkred") +
  # Add regression equation
  annotate("text", x = min(penguins_clean$bill_length_mm) + 1,
           y = max(penguins_clean$body_mass_g) - 1000,
           label = paste0("y = ", round(coef(model)[1], 1), " + ",
                         round(coef(model)[2], 1), "x"),
           hjust = 0, size = 4, color = "darkred") +
  # Add professional labels
  labs(
    title = "Relationship Between Bill Length and Body Mass",
    subtitle = "Linear regression analysis shows positive correlation with species differences",
    x = "Bill Length (mm)",
    y = "Body Mass (g)",
    color = "Species",
    caption = "Data source: Palmer Penguins dataset"
  ) +
  # Use a colorblind-friendly palette
  scale_color_viridis_d() +
  # Adjust axis limits for better visualization
  coord_cartesian(expand = TRUE)

# Create diagnostic plots using the performance package
check_model <- check_model(model)
plot(check_model)

# Create additional diagnostic plots for specific issues
par(mfrow = c(2, 2))
plot(model)
```

::: {.callout-note}
## Code Explanation

This code demonstrates linear regression analysis:

1. **Model Setup**:
   - Uses `lm()` for linear regression
   - Predicts body mass from bill length
   - Includes model diagnostics

2. **Visualization**:
   - Creates scatter plot with regression line
   - Uses `geom_smooth()` for trend line
   - Adds appropriate labels

3. **Diagnostics**:
   - Residual plots
   - Q-Q plot
   - Scale-location plot
   - Leverage plot
:::

::: {.callout-important}
## Results Interpretation

The regression analysis reveals:

1. **Model Fit**:
   - Strength of relationship (R²)
   - Statistical significance (p-value)
   - Direction of relationship

2. **Assumptions**:
   - Linearity of relationship
   - Homogeneity of variance
   - Normality of residuals
   - Independence of observations

3. **Practical Significance**:
   - Effect size
   - Biological relevance
   - Prediction accuracy
:::

::: {.callout-tip}
## PROFESSIONAL TIP: Regression Analysis Best Practices

When conducting regression analysis:

1. **Model Selection**:
   - Choose appropriate model type
   - Consider variable transformations
   - Check for multicollinearity
   - Evaluate model assumptions

2. **Diagnostic Checks**:
   - Examine residual plots
   - Check for outliers
   - Verify normality
   - Assess leverage points

3. **Reporting**:
   - Include model coefficients
   - Report confidence intervals
   - Provide effect sizes
   - Discuss limitations
:::

## Multiple Regression

Multiple regression extends linear regression to include multiple predictors:

```{r}
# Load required packages
library(broom)
library(knitr)
library(dplyr)

# Create multiple regression model
multi_model <- lm(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species,
                 data = penguins_clean)

# Get model summary with broom for cleaner output
multi_summary <- summary(multi_model)
multi_tidy <- tidy(multi_model, conf.int = TRUE)
multi_glance <- glance(multi_model)

# Display key model metrics
cat("Multiple Regression Model Summary:\n")
cat(paste0("R² = ", round(multi_glance$r.squared, 3),
          ", Adjusted R² = ", round(multi_glance$adj.r.squared, 3), "\n"))
cat(paste0("F-statistic: ", round(multi_glance$statistic, 2),
          " on ", multi_glance$df, " and ", multi_glance$df.residual,
          " DF, p-value: ", format.pval(multi_glance$p.value, digits = 3), "\n\n"))

# Create a simple table of coefficients
print(multi_tidy)

# Check for multicollinearity
library(car)
vif_values <- car::vif(multi_model)
knitr::kable(vif_values, digits = 3,
             caption = "Variance Inflation Factors (VIF)")

# Check for model assumptions
check_multi_model <- check_model(multi_model)
plot(check_multi_model)

# Visualize predictor effects
library(effects)
plot(allEffects(multi_model), ask = FALSE)

# Compare models using base R
model_comparison <- data.frame(
  Metric = c("R²", "Adjusted R²", "AIC", "BIC", "N"),
  `Simple Model` = c(
    round(summary(model)$r.squared, 3),
    round(summary(model)$adj.r.squared, 3),
    round(AIC(model), 1),
    round(BIC(model), 1),
    nobs(model)
  ),
  `Multiple Model` = c(
    round(summary(multi_model)$r.squared, 3),
    round(summary(multi_model)$adj.r.squared, 3),
    round(AIC(multi_model), 1),
    round(BIC(multi_model), 1),
    nobs(multi_model)
  ),
  check.names = FALSE
)
knitr::kable(model_comparison, caption = "Comparison of Regression Models")

# Create a visualization of predicted vs. actual values
predicted_values <- augment(multi_model, data = penguins_clean)

ggplot(predicted_values, aes(x = .fitted, y = body_mass_g, color = species)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray50") +
  labs(
    title = "Predicted vs. Actual Body Mass",
    subtitle = "Points closer to the dashed line indicate better predictions",
    x = "Predicted Body Mass (g)",
    y = "Actual Body Mass (g)",
    color = "Species",
    caption = "Model: body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species"
  ) +
  scale_color_viridis_d() +
  theme(legend.position = "bottom")

# Create a partial dependence plot for flipper length
pdp_flipper <- ggeffects::ggpredict(multi_model, terms = "flipper_length_mm")
plot(pdp_flipper) +
  labs(
    title = "Effect of Flipper Length on Body Mass",
    subtitle = "Controlling for other variables in the model",
    caption = "Shaded area represents 95% confidence interval"
  )
```

::: {.callout-note}
## Code Explanation

This code demonstrates enhanced multiple regression analysis techniques:

1. **Model Construction**
   - Uses `lm()` to build a multiple regression with morphological predictors and species
   - Creates a more comprehensive model accounting for both measurements and taxonomy
   - Properly handles categorical predictors (species) with appropriate contrasts

2. **Advanced Diagnostics**
   - Evaluates multicollinearity with Variance Inflation Factors (VIF)
   - Conducts comprehensive model assumption checks
   - Compares model performance metrics across simple and multiple regression

3. **Professional Visualization**
   - Creates an elegant predicted vs. actual plot to assess model fit
   - Generates partial dependence plots to visualize individual predictor effects
   - Uses model effects plots to show relationships while controlling for other variables
   - Implements consistent styling with appropriate annotations and colorblind-friendly palettes
:::

::: {.callout-important}
## Results Interpretation

The multiple regression analysis reveals several important insights:

1. **Model Performance**
   - Multiple regression substantially improves explanatory power over simple regression
   - The adjusted R² is much higher, indicating better model fit
   - Species is a significant predictor, suggesting morphological differences between species

2. **Predictor Effects**
   - Flipper length and bill length both positively correlate with body mass
   - Species-specific effects indicate evolutionary differences in body size
   - Bill depth shows a weaker relationship when controlling for other variables

3. **Diagnostics Findings**
   - Multicollinearity (VIF values) appears manageable (VIF < 5 is generally acceptable)
   - The model generally meets assumptions for inference
   - Residual patterns suggest the linear model captures the main relationships well
:::

::: {.callout-tip}
## PROFESSIONAL TIP: Multiple Regression Best Practices

When conducting ecological multiple regression analyses:

1. **Model Building Strategy**
   - Start with biologically meaningful predictors based on theory
   - Consider alternative model specifications (linear, polynomial, interactions)
   - Use a hierarchical approach, starting simple and adding complexity
   - Adopt information-theoretic approaches (AIC/BIC) for model selection

2. **Addressing Collinearity**
   - Examine correlations among predictors before modeling
   - Calculate VIF values and remove highly collinear predictors (VIF > 10)
   - Consider dimension reduction techniques (PCA) for related predictors
   - Use regularization methods (ridge, lasso) for high-dimensional data

3. **Results Communication**
   - Present standardized coefficients to compare predictor importance
   - Visualize partial effects rather than just reporting coefficients
   - Report effect sizes and confidence intervals, not just p-values
   - Describe both statistical and biological significance of your findings
:::

## Logistic Regression

Logistic regression models binary outcomes:

```{r}
# Prepare data for logistic regression by creating a binary outcome
# We'll predict whether a penguin is Adelie species or not
penguins_binary <- penguins_clean %>%
  # Create a binary outcome variable (is_adelie)
  mutate(is_adelie = ifelse(species == "Adelie", 1, 0),
         # Convert to factor for better model interpretation
         is_adelie_factor = factor(is_adelie, levels = c(0, 1),
                                  labels = c("Other", "Adelie")))

# Create logistic regression model
log_model <- glm(is_adelie ~ bill_length_mm + bill_depth_mm + flipper_length_mm,
                family = binomial(link = "logit"),
                data = penguins_binary)

# Summarize model with tidy output
log_summary <- summary(log_model)
log_tidy <- tidy(log_model, conf.int = TRUE, exponentiate = TRUE)
log_glance <- glance(log_model)

# Display key model metrics
cat("Logistic Regression Model Summary:\n")
cat(paste0("AIC: ", round(log_glance$AIC, 2),
          ", Deviance: ", round(log_glance$deviance, 2), "\n"))
cat(paste0("Null Deviance: ", round(log_glance$null.deviance, 2),
          ", DF: ", log_glance$df.null, ", Residual DF: ", log_glance$df.residual, "\n\n"))

# Calculate and show pseudo R-squared (McFadden)
pseudo_r2 <- 1 - (log_glance$deviance / log_glance$null.deviance)
cat(paste0("McFadden's Pseudo R²: ", round(pseudo_r2, 3), "\n\n"))

# Create a table of odds ratios with CI
log_tidy %>%
  gt() %>%
  tab_header(title = "Logistic Regression Results (Odds Ratios)") %>%
  fmt_number(columns = c("estimate", "std.error", "statistic", "p.value", "conf.low", "conf.high"), decimals = 3)

# Add predictions to the data
penguins_pred <- penguins_binary %>%
  mutate(
    predicted_prob = predict(log_model, type = "response"),
    predicted_class = ifelse(predicted_prob > 0.5, "Adelie", "Other"),
    correct = ifelse(predicted_class == ifelse(is_adelie == 1, "Adelie", "Other"), "Correct", "Incorrect")
  )

# Create a confusion matrix
confusion <- table(
  Predicted = penguins_pred$predicted_class,
  Actual = ifelse(penguins_pred$is_adelie == 1, "Adelie", "Other")
)

# Calculate accuracy metrics
accuracy <- sum(diag(confusion)) / sum(confusion)
sensitivity <- confusion["Adelie", "Adelie"] / sum(confusion[, "Adelie"])
specificity <- confusion["Other", "Other"] / sum(confusion[, "Other"])
precision <- confusion["Adelie", "Adelie"] / sum(confusion["Adelie", ])

# Display confusion matrix with metrics
knitr::kable(confusion,
            caption = paste0("Confusion Matrix (Accuracy = ", round(accuracy * 100, 1), "%)"))

cat(paste0("Sensitivity (True Positive Rate): ", round(sensitivity * 100, 1), "%\n"))
cat(paste0("Specificity (True Negative Rate): ", round(specificity * 100, 1), "%\n"))
cat(paste0("Precision (Positive Predictive Value): ", round(precision * 100, 1), "%\n\n"))

# Create ROC curve
library(pROC)
roc_curve <- roc(penguins_binary$is_adelie, fitted(log_model))
auc_value <- auc(roc_curve)

# Plot the ROC curve
plot(roc_curve, print.auc = TRUE, auc.polygon = TRUE,
     grid = TRUE, main = "ROC Curve for Adelie Penguin Classification")

# Create a visualization of predicted probabilities by species
ggplot(penguins_pred, aes(x = bill_length_mm, y = bill_depth_mm, color = predicted_prob)) +
  geom_point(size = 3, alpha = 0.7) +
  # Add decision boundary (0.5 probability contour)
  geom_contour(aes(z = predicted_prob), breaks = 0.5, color = "black", linewidth = 1) +
  # Add text labels for misclassified points
  geom_text(data = filter(penguins_pred, correct == "Incorrect"),
            aes(label = "✗"), color = "black", size = 4, nudge_y = 0.5) +
  # Color gradient for probability
  scale_color_gradient2(low = "navy", mid = "white", high = "red",
                      midpoint = 0.5, limits = c(0, 1)) +
  facet_wrap(~species) +
  labs(
    title = "Classification of Adelie vs. Other Penguins",
    subtitle = paste0("AUC = ", round(auc_value, 3), ", Accuracy = ", round(accuracy * 100, 1), "%"),
    x = "Bill Length (mm)",
    y = "Bill Depth (mm)",
    color = "Probability\nof Adelie",
    caption = "Black line: decision boundary (p=0.5), ✗: misclassified points"
  ) +
  theme(legend.position = "right")

# Create marginal effects plots for the predictors
library(effects)
plot(allEffects(log_model), ask = FALSE, main = "Marginal Effects on Probability of Adelie")

# Check model fit
library(DHARMa)
sim_residuals <- simulateResiduals(log_model)
plot(sim_residuals)
```

::: {.callout-note}
## Code Explanation

This enhanced logistic regression analysis provides comprehensive insights:

1. **Model Construction**
   - Creates a binary outcome variable (Adelie vs. Other species)
   - Uses `glm()` with a binomial family and logit link
   - Incorporates multiple predictors (bill length, bill depth, flipper length)

2. **Professional Reporting**
   - Displays odds ratios with confidence intervals for interpretability
   - Calculates and reports pseudo-R² (McFadden) for model fit
   - Creates a detailed confusion matrix with classification metrics
   - Generates a ROC curve with AUC for overall discriminative ability

3. **Advanced Visualization**
   - Plots predicted probabilities with decision boundaries
   - Highlights misclassified points for error analysis
   - Creates marginal effects plots showing how each predictor influences classification
   - Conducts simulation-based residual analysis for model validation
:::

::: {.callout-important}
## Results Interpretation

The logistic regression yields important ecological insights:

1. **Classification Performance**
   - The model effectively distinguishes Adelie penguins from other species
   - High AUC (area under ROC curve) indicates strong discriminative ability
   - Classification accuracy, sensitivity, and specificity show the practical utility

2. **Morphological Predictors**
   - Bill dimensions are strong predictors of species identity
   - Odds ratios reveal how changes in morphology affect classification probability
   - Interaction between bill length and depth creates a clear decision boundary

3. **Ecological Implications**
   - The model demonstrates morphological differentiation between penguin species
   - Misclassified individuals may represent morphological overlap zones
   - Results align with evolutionary theory on character displacement
   - Potential applications in field identification and evolutionary studies
:::

::: {.callout-tip}
## PROFESSIONAL TIP: Logistic Regression Best Practices

When applying logistic regression in ecological studies:

1. **Model Development**
   - Balance the number of events and predictors (aim for >10 events per predictor)
   - Test for nonlinearity in continuous predictors using splines or polynomial terms
   - Evaluate interactions between predictors where biologically meaningful
   - Consider regularization for high-dimensional data to prevent overfitting

2. **Model Evaluation**
   - Use cross-validation to assess predictive performance
   - Report multiple performance metrics beyond p-values (AUC, accuracy, sensitivity, specificity)
   - Consider the costs of different error types (false positives vs. false negatives)
   - Test model calibration (agreement between predicted probabilities and observed outcomes)

3. **Result Communication**
   - Report odds ratios for interpretability (not just coefficients)
   - Create visualizations showing decision boundaries and classification regions
   - Discuss practical significance for ecological applications
   - Indicate model limitations and potential sampling biases
:::

## Summary

In this chapter, we've explored regression analysis using both traditional base R approaches and the modern tidymodels framework:

- **Linear regression** for modeling continuous relationships between morphological variables
- **Multiple regression** for incorporating several predictors and controlling for confounding factors
- **Logistic regression** for binary classification and probability estimation
- **Tidymodels workflow** for consistent, reproducible modeling

The tidymodels framework provides:
- Unified interface across different model types
- Consistent preprocessing with recipes
- Built-in resampling and cross-validation
- Standardized model evaluation metrics
- Streamlined workflow management

Each approach provides unique insights into ecological patterns and relationships, with applications ranging from morphological studies to species classification and trait prediction.

## Regression with Tidymodels Framework

The tidymodels ecosystem provides a modern, unified approach to statistical modeling in R. Let's explore how to implement regression analysis using this framework.

### Setting Up Tidymodels

```{r}
# Load tidymodels packages
library(tidymodels)  # Meta-package loading parsnip, recipes, rsample, tune, workflows, yardstick
library(tidyverse)    # For data manipulation

# Set random seed for reproducibility
set.seed(123)

# Load and prepare data
penguins <- read_csv("../data/environmental/climate_data.csv", show_col_types = FALSE) %>%
  filter(!is.na(bill_length_mm), !is.na(body_mass_g),
         !is.na(bill_depth_mm), !is.na(flipper_length_mm)) %>%
  mutate(species = as.factor(species))

# Display data structure
glimpse(penguins)
```

::: {.callout-note}
## Tidymodels Philosophy

The tidymodels framework follows key principles:

1. **Consistency**: Uniform syntax across different model types
2. **Modularity**: Separate components for different modeling tasks
3. **Tidyverse integration**: Works seamlessly with dplyr, ggplot2, etc.
4. **Reproducibility**: Built-in support for resampling and validation
5. **Extensibility**: Easy to add new models and methods
:::

### Linear Regression with Tidymodels

```{r}
# Step 1: Split data into training and testing sets
penguin_split <- initial_split(penguins, prop = 0.75, strata = species)
penguin_train <- training(penguin_split)
penguin_test <- testing(penguin_split)

cat("Training set:", nrow(penguin_train), "observations\n")
cat("Testing set:", nrow(penguin_test), "observations\n")

# Step 2: Define a recipe for preprocessing
penguin_recipe <- recipe(body_mass_g ~ bill_length_mm + bill_depth_mm +
                         flipper_length_mm + species,
                         data = penguin_train) %>%
  # Create dummy variables for species
  step_dummy(species) %>%
  # Normalize all numeric predictors
  step_normalize(all_numeric_predictors()) %>%
  # Check for zero variance predictors
  step_zv(all_predictors())

# View the recipe
print(penguin_recipe)

# Step 3: Specify the model
lm_spec <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

print(lm_spec)

# Step 4: Create a workflow combining recipe and model
penguin_wf <- workflow() %>%
  add_recipe(penguin_recipe) %>%
  add_model(lm_spec)

print(penguin_wf)

# Step 5: Fit the model
penguin_fit <- penguin_wf %>%
  fit(data = penguin_train)

# Extract and display model results
tidy(penguin_fit) %>%
  knitr::kable(caption = "Tidymodels Linear Regression Coefficients",
               digits = 3)

# Get model performance metrics on training data
train_results <- penguin_fit %>%
  predict(penguin_train) %>%
  bind_cols(penguin_train) %>%
  metrics(truth = body_mass_g, estimate = .pred)

knitr::kable(train_results,
             caption = "Training Set Performance Metrics",
             digits = 3)

# Step 6: Evaluate on test data
test_results <- penguin_fit %>%
  predict(penguin_test) %>%
  bind_cols(penguin_test) %>%
  metrics(truth = body_mass_g, estimate = .pred)

knitr::kable(test_results,
             caption = "Test Set Performance Metrics",
             digits = 3)

# Create predictions for visualization
penguin_pred <- penguin_fit %>%
  predict(penguin_test) %>%
  bind_cols(penguin_test)

# Visualize predictions vs. actual
ggplot(penguin_pred, aes(x = body_mass_g, y = .pred, color = species)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray50") +
  scale_color_viridis_d() +
  labs(
    title = "Tidymodels: Predicted vs. Actual Body Mass",
    subtitle = "Test set predictions from linear regression workflow",
    x = "Actual Body Mass (g)",
    y = "Predicted Body Mass (g)",
    color = "Species",
    caption = "Dashed line represents perfect predictions"
  ) +
  theme_minimal()
```

::: {.callout-important}
## Tidymodels Workflow Benefits

This tidymodels approach provides several advantages:

1. **Clear separation of concerns**: Data splitting, preprocessing, and modeling are distinct steps
2. **Automatic train/test split**: Built-in support for validation
3. **Preprocessing pipeline**: Recipe ensures consistent transformations
4. **Reusable workflows**: Easy to apply the same pipeline to new data
5. **Standardized metrics**: Consistent evaluation across models

The test set RMSE and R² provide unbiased estimates of model performance on new data.
:::

### Cross-Validation with Tidymodels

```{r}
# Create cross-validation folds
set.seed(456)
penguin_folds <- vfold_cv(penguin_train, v = 10, strata = species)

cat("Created", nrow(penguin_folds), "cross-validation folds\n")

# Fit model using cross-validation
cv_results <- penguin_wf %>%
  fit_resamples(
    resamples = penguin_folds,
    control = control_resamples(save_pred = TRUE),
    metrics = metric_set(rmse, rsq, mae)
  )

# Collect and display metrics
cv_metrics <- collect_metrics(cv_results)

knitr::kable(cv_metrics,
             caption = "10-Fold Cross-Validation Performance",
             digits = 3)

# Visualize cross-validation results
collect_metrics(cv_results, summarize = FALSE) %>%
  ggplot(aes(x = .metric, y = .estimate)) +
  geom_boxplot(fill = "skyblue", alpha = 0.7) +
  geom_jitter(width = 0.1, alpha = 0.5, size = 2) +
  labs(
    title = "Cross-Validation Performance Distribution",
    subtitle = "10-fold CV shows model stability across folds",
    x = "Metric",
    y = "Value"
  ) +
  theme_minimal()

# Extract predictions from all folds
cv_predictions <- collect_predictions(cv_results)

# Plot predictions from cross-validation
ggplot(cv_predictions, aes(x = body_mass_g, y = .pred)) +
  geom_point(alpha = 0.5, color = "steelblue") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Cross-Validation Predictions",
    subtitle = "All predictions from 10-fold CV",
    x = "Actual Body Mass (g)",
    y = "Predicted Body Mass (g)"
  ) +
  theme_minimal()
```

::: {.callout-tip}
## PROFESSIONAL TIP: Why Cross-Validation?

Cross-validation provides:

1. **More reliable performance estimates**: Averages over multiple train/test splits
2. **Better use of data**: All observations used for both training and validation
3. **Overfitting detection**: Large difference between training and CV metrics suggests overfitting
4. **Model comparison**: Fair basis for comparing different modeling approaches
5. **Hyperparameter tuning**: Essential for selecting optimal model parameters
:::

### Model Comparison with Tidymodels

```{r}
# Define multiple model specifications
lm_spec <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

rf_spec <- rand_forest(trees = 100) %>%
  set_engine("ranger") %>%
  set_mode("regression")

# Create workflows for each model
lm_wf <- workflow() %>%
  add_recipe(penguin_recipe) %>%
  add_model(lm_spec)

rf_wf <- workflow() %>%
  add_recipe(penguin_recipe) %>%
  add_model(rf_spec)

# Fit both models with cross-validation
set.seed(789)
lm_cv <- lm_wf %>%
  fit_resamples(
    resamples = penguin_folds,
    metrics = metric_set(rmse, rsq, mae)
  )

rf_cv <- rf_wf %>%
  fit_resamples(
    resamples = penguin_folds,
    metrics = metric_set(rmse, rsq, mae)
  )

# Compare model performance
model_comparison <- bind_rows(
  collect_metrics(lm_cv) %>% mutate(model = "Linear Regression"),
  collect_metrics(rf_cv) %>% mutate(model = "Random Forest")
)

knitr::kable(model_comparison %>% select(model, .metric, mean, std_err),
             caption = "Model Comparison: Linear Regression vs. Random Forest",
             digits = 3,
             col.names = c("Model", "Metric", "Mean", "Std Error"))

# Visualize model comparison
model_comparison %>%
  ggplot(aes(x = model, y = mean, fill = model)) +
  geom_col() +
  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err),
                width = 0.2) +
  facet_wrap(~ .metric, scales = "free_y") +
  scale_fill_viridis_d(begin = 0.3, end = 0.8) +
  labs(
    title = "Model Performance Comparison",
    subtitle = "Cross-validation results with standard errors",
    x = "Model Type",
    y = "Performance (mean ± SE)"
  ) +
  theme_minimal() +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 45, hjust = 1))

### Hyperparameter Tuning

Optimizing model parameters is crucial for machine learning models like Random Forest.

```{r}
# Define a model specification with tunable parameters
rf_tune_spec <- rand_forest(
  trees = 1000,
  mtry = tune(),
  min_n = tune()
) %>%
  set_engine("ranger") %>%
  set_mode("regression")

# Create a workflow with the tunable model
rf_tune_wf <- workflow() %>%
  add_recipe(penguin_recipe) %>%
  add_model(rf_tune_spec)

# Define the grid of parameter values to try
rf_grid <- grid_regular(
  mtry(range = c(1, 5)),
  min_n(range = c(2, 10)),
  levels = 5
)

# Tune the model using cross-validation
# Note: This may take a moment to run
set.seed(345)
rf_tune_res <- tune_grid(
  rf_tune_wf,
  resamples = penguin_folds,
  grid = rf_grid,
  metrics = metric_set(rmse, rsq)
)

# Collect tuning metrics
rf_tune_res %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  pivot_longer(cols = c(mtry, min_n), names_to = "parameter", values_to = "value") %>%
  ggplot(aes(x = value, y = mean, color = parameter)) +
  geom_point() +
  geom_line() +
  facet_wrap(~parameter, scales = "free_x") +
  labs(title = "Hyperparameter Tuning Results", y = "RMSE (lower is better)") +
  theme_minimal()

# Select the best parameters
best_rf <- select_best(rf_tune_res, metric = "rmse")
print(best_rf)

# Finalize the workflow with the best parameters
final_rf_wf <- finalize_workflow(rf_tune_wf, best_rf)

# Fit the final model to the full training set and evaluate on test set
final_fit <- last_fit(final_rf_wf, penguin_split)

# Show final performance
collect_metrics(final_fit)
```

::: {.callout-tip}
## Tuning Strategy
Grid search explores a defined set of parameters. For more complex models, consider `tune_bayes()` for Bayesian optimization which can be more efficient.
:::

## Model Interpretation and Explainability

While machine learning models like Random Forest can achieve high predictive accuracy, understanding *why* they make specific predictions is crucial for ecological applications. Model interpretation helps us extract ecological insights and communicate results to stakeholders.

### Variable Importance

```{r}
# Load packages for model interpretation
library(DALEX)
library(DALEXtra)
library(tidymodels)

# Use the final Random Forest model from earlier
# Create an explainer object
explainer_rf <- explain_tidymodels(
  final_fit,
  data = penguin_test %>% select(-body_mass_g),
  y = penguin_test$body_mass_g,
  label = "Random Forest",
  verbose = FALSE
)

# Calculate variable importance using permutation
vi_rf <- model_parts(explainer_rf, loss_function = loss_root_mean_square)

# Visualize variable importance
plot(vi_rf) +
  labs(title = "Variable Importance for Body Mass Prediction",
       subtitle = "Permutation-based importance shows contribution of each predictor") +
  theme_minimal()

# Get the importance values
vi_df <- as.data.frame(vi_rf)
vi_summary <- vi_df %>%
  filter(variable != "_baseline_", variable != "_full_model_") %>%
  group_by(variable) %>%
  summarize(
    mean_dropout_loss = mean(dropout_loss),
    .groups = "drop"
  ) %>%
  arrange(desc(mean_dropout_loss))

print(vi_summary)
```

::: {.callout-note}
## Code Explanation

This code demonstrates model-agnostic variable importance:

1. **DALEX Framework**
   - Creates an "explainer" object that wraps the tidymodels workflow
   - Provides a unified interface for model interpretation
   - Works with any model type (linear, tree-based, neural networks)

2. **Permutation Importance**
   - Measures how much model performance decreases when a variable is randomly shuffled
   - Higher dropout loss = more important variable
   - Unlike tree-based importance, this works for any model and accounts for correlations

3. **Interpretation**
   - Variables are ranked by their contribution to predictions
   - Helps identify which morphological features are most predictive of body mass
   - Provides ecological insights into species morphology
:::

### Partial Dependence Plots

Partial dependence plots show how predictions change as a single variable varies, while averaging over all other variables.

```{r}
# Create partial dependence profiles for key variables
pdp_flipper <- model_profile(
  explainer_rf,
  variables = "flipper_length_mm",
  N = NULL  # Use all observations
)

pdp_bill <- model_profile(
  explainer_rf,
  variables = "bill_length_mm",
  N = NULL
)

# Plot partial dependence
plot(pdp_flipper) +
  labs(title = "Partial Dependence: Flipper Length",
       subtitle = "Shows average effect of flipper length on predicted body mass",
       x = "Flipper Length (mm)",
       y = "Predicted Body Mass (g)") +
  theme_minimal()

plot(pdp_bill) +
  labs(title = "Partial Dependence: Bill Length",
       subtitle = "Shows average effect of bill length on predicted body mass",
       x = "Bill Length (mm)",
       y = "Predicted Body Mass (g)") +
  theme_minimal()

# Create 2D partial dependence plot for interactions
pdp_2d <- model_profile(
  explainer_rf,
  variables = c("flipper_length_mm", "bill_length_mm"),
  N = 100
)

# Note: 2D plots require additional processing
# For simplicity, we'll show individual effects
```

::: {.callout-important}
## Results Interpretation

Partial dependence plots reveal important ecological relationships:

1. **Marginal Effects**
   - Shows how each predictor affects the response on average
   - Accounts for correlations between predictors
   - Reveals non-linear relationships that linear models would miss

2. **Ecological Insights**
   - Flipper length typically shows a strong positive relationship with body mass
   - The relationship may be non-linear (e.g., diminishing returns at large sizes)
   - Different species may show different patterns (captured by species variable)

3. **Model Behavior**
   - Helps validate that the model learned sensible relationships
   - Can reveal unexpected patterns that warrant further investigation
   - Useful for communicating model predictions to non-technical audiences
:::

### Individual Conditional Expectation (ICE) Plots

While partial dependence shows average effects, ICE plots show how predictions change for individual observations.

```{r}
# Create ICE plots for flipper length
ice_flipper <- model_profile(
  explainer_rf,
  variables = "flipper_length_mm",
  N = 50,  # Use 50 observations for clarity
  type = "conditional"
)

# Plot ICE curves
plot(ice_flipper) +
  labs(title = "Individual Conditional Expectation: Flipper Length",
       subtitle = "Each line shows how prediction changes for one penguin",
       x = "Flipper Length (mm)",
       y = "Predicted Body Mass (g)") +
  theme_minimal()

# Compare with partial dependence (average)
plot(ice_flipper, geom = "profiles") +
  labs(title = "ICE Plots with Partial Dependence (yellow line)",
       subtitle = "Individual effects vs. average effect",
       x = "Flipper Length (mm)",
       y = "Predicted Body Mass (g)") +
  theme_minimal()
```

::: {.callout-tip}
## PROFESSIONAL TIP: Choosing Interpretation Methods

When interpreting machine learning models for ecological applications:

1. **Variable Importance**
   - Use permutation importance for model-agnostic assessment
   - Compare with domain knowledge to validate model behavior
   - Consider both statistical and ecological importance
   - Report confidence intervals when possible

2. **Partial Dependence Plots**
   - Show average marginal effects of predictors
   - Useful for understanding overall patterns
   - Can mask heterogeneous effects across observations
   - Best for communicating general relationships

3. **ICE Plots**
   - Reveal individual-level heterogeneity
   - Identify subgroups with different response patterns
   - More complex to interpret than PDP
   - Useful for detecting interactions

4. **Break-Down Plots** (for individual predictions)
   - Explain specific predictions step-by-step
   - Useful for understanding outliers or unusual cases
   - Helps build trust in model predictions
   - Essential for high-stakes conservation decisions
:::

### Explaining Individual Predictions

For specific conservation decisions, we often need to understand why the model made a particular prediction.

```{r}
# Select an interesting observation to explain
observation_to_explain <- penguin_test[1, ]

cat("Explaining prediction for:\n")
print(observation_to_explain %>% select(species, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g))

# Create a break-down explanation
bd <- predict_parts(
  explainer_rf,
  new_observation = observation_to_explain,
  type = "break_down"
)

# Plot the break-down
plot(bd) +
  labs(title = "Break-Down Plot for Individual Prediction",
       subtitle = "Shows contribution of each variable to the final prediction") +
  theme_minimal()

# Create SHAP values for more robust attribution
shap <- predict_parts(
  explainer_rf,
  new_observation = observation_to_explain,
  type = "shap",
  B = 25  # Number of random orderings
)

# Plot SHAP values
plot(shap) +
  labs(title = "SHAP Values for Individual Prediction",
       subtitle = "Average contribution across all possible variable orderings") +
  theme_minimal()
```

::: {.callout-note}
## Code Explanation

Individual prediction explanations help understand specific cases:

1. **Break-Down Plots**
   - Show step-by-step how each variable contributes to the prediction
   - Start from the average prediction and add each variable's effect
   - Order matters: variables are added in order of importance

2. **SHAP Values**
   - Shapley Additive exPlanations from game theory
   - Average contribution across all possible orderings of variables
   - More robust than break-down plots but computationally intensive
   - Provides fair attribution of prediction to each feature

3. **Ecological Applications**
   - Explain why a particular species is predicted in a certain location
   - Understand which factors drive high/low abundance predictions
   - Communicate model reasoning to stakeholders and decision-makers
   - Identify data quality issues or model failures
:::

::: {.callout-important}
## Communicating ML Results in Ecology

When presenting machine learning results to ecological audiences:

1. **Focus on Ecological Meaning**
   - Translate statistical importance to ecological significance
   - Connect model patterns to known ecological processes
   - Discuss biological plausibility of relationships
   - Relate findings to conservation implications

2. **Visualize Effectively**
   - Use partial dependence plots for main effects
   - Show uncertainty in predictions
   - Include actual data points alongside model predictions
   - Use colors and labels that resonate with ecological context

3. **Acknowledge Limitations**
   - Discuss what the model cannot capture (e.g., biotic interactions)
   - Explain extrapolation risks
   - Mention data quality and sampling bias issues
   - Suggest validation approaches

4. **Provide Actionable Insights**
   - Link predictions to management recommendations
   - Identify key variables that can be monitored or manipulated
   - Suggest priority areas for conservation action
   - Propose adaptive management strategies based on model uncertainty
:::

Model interpretation transforms "black box" machine learning into a tool for ecological understanding. By explaining how models make predictions, we gain insights into ecological processes and build confidence in using ML for conservation decision-making.

## Time Series Analysis for Ecological Data

Time series data—observations collected sequentially over time—are common in ecology. Population counts, climate measurements, and phenological observations all have temporal structure that standard regression methods cannot properly handle.

### Understanding Temporal Autocorrelation

```{r}
#| warning: false
#| message: false

# Load packages for time series analysis
library(forecast)
library(tsibble)
library(feasts)
library(ggplot2)
library(dplyr)

# Create simulated population time series
set.seed(2024)
years <- 1990:2023
n_years <- length(years)

# Simulate population with trend, seasonality, and noise
trend <- 100 + 2 * (1:n_years)  # Increasing trend
seasonal <- 10 * sin(2 * pi * (1:n_years) / 5)  # 5-year cycle
noise <- rnorm(n_years, 0, 5)
population <- trend + seasonal + noise

# Create time series data frame
pop_ts_data <- data.frame(
  year = years,
  population = round(population)
)

# Visualize the time series
ggplot(pop_ts_data, aes(x = year, y = population)) +
  geom_line(color = "steelblue", linewidth = 1) +
  geom_point(color = "steelblue", size = 2) +
  labs(
    title = "Simulated Population Time Series",
    subtitle = "Shows trend and cyclic pattern typical of ecological data",
    x = "Year",
    y = "Population Size"
  ) +
  theme_minimal()

# Test for temporal autocorrelation
# Create ACF and PACF plots
par(mfrow = c(2, 1))
acf(pop_ts_data$population, main = "Autocorrelation Function (ACF)")
pacf(pop_ts_data$population, main = "Partial Autocorrelation Function (PACF)")
```

::: {.callout-note}
## Code Explanation

This code introduces time series concepts for ecological data:

1. **Time Series Components**
   - **Trend**: Long-term increase or decrease (e.g., climate change effects)
   - **Seasonality**: Regular cyclic patterns (e.g., annual breeding cycles)
   - **Noise**: Random variation (e.g., environmental stochasticity)

2. **Autocorrelation Functions**
   - **ACF**: Shows correlation between observations at different time lags
   - **PACF**: Shows direct correlation after removing indirect effects
   - Significant spikes indicate temporal dependence

3. **Ecological Relevance**
   - Population dynamics often show autocorrelation due to age structure
   - Climate variables have strong seasonal patterns
   - Ignoring autocorrelation leads to invalid statistical inferences
:::

### Time Series Decomposition

```{r}
# Convert to ts object for decomposition
pop_ts <- ts(pop_ts_data$population, start = 1990, frequency = 1)

# Decompose the time series
decomp <- stl(ts(pop_ts_data$population, frequency = 5), s.window = "periodic")

# Plot decomposition
plot(decomp, main = "Time Series Decomposition")

# Extract components
trend_component <- decomp$time.series[, "trend"]
seasonal_component <- decomp$time.series[, "seasonal"]
remainder <- decomp$time.series[, "remainder"]

# Create a data frame for ggplot
decomp_df <- data.frame(
  year = years,
  observed = pop_ts_data$population,
  trend = as.numeric(trend_component),
  seasonal = as.numeric(seasonal_component),
  remainder = as.numeric(remainder)
)

# Visualize components with ggplot
library(tidyr)
decomp_long <- decomp_df %>%
  pivot_longer(cols = c(observed, trend, seasonal, remainder),
               names_to = "component",
               values_to = "value")

ggplot(decomp_long, aes(x = year, y = value)) +
  geom_line(color = "steelblue") +
  facet_wrap(~component, scales = "free_y", ncol = 1) +
  labs(
    title = "Time Series Decomposition Components",
    x = "Year",
    y = "Value"
  ) +
  theme_minimal()
```

::: {.callout-important}
## Results Interpretation

Time series decomposition reveals the structure of temporal data:

1. **Trend Component**
   - Shows the long-term direction of the population
   - Increasing trend suggests population growth
   - Can be linear or non-linear
   - Important for long-term conservation planning

2. **Seasonal Component**
   - Reveals cyclic patterns in the data
   - In this example, shows a 5-year cycle
   - Could represent environmental cycles (e.g., El Niño)
   - Helps identify optimal monitoring times

3. **Remainder (Residuals)**
   - Random variation after removing trend and seasonality
   - Should be approximately white noise if decomposition is appropriate
   - Large residuals may indicate unusual events or data quality issues

4. **Ecological Applications**
   - Separate long-term trends from natural cycles
   - Identify critical periods in species life cycles
   - Detect anomalies or regime shifts
   - Inform sampling design for monitoring programs
:::

### Forecasting with ARIMA Models

```{r}
# Fit an ARIMA model
# auto.arima selects the best model automatically
arima_model <- auto.arima(pop_ts)

# Display model summary
summary(arima_model)

# Make forecasts for the next 5 years
forecast_result <- forecast(arima_model, h = 5)

# Plot the forecast
autoplot(forecast_result) +
  labs(
    title = "Population Forecast (ARIMA Model)",
    subtitle = "5-year forecast with 80% and 95% confidence intervals",
    x = "Year",
    y = "Population Size"
  ) +
  theme_minimal()

# Extract forecast values
forecast_df <- data.frame(
  year = 2024:2028,
  point_forecast = as.numeric(forecast_result$mean),
  lower_80 = as.numeric(forecast_result$lower[, 1]),
  upper_80 = as.numeric(forecast_result$upper[, 1]),
  lower_95 = as.numeric(forecast_result$lower[, 2]),
  upper_95 = as.numeric(forecast_result$upper[, 2])
)

print(forecast_df)
```

::: {.callout-tip}
## PROFESSIONAL TIP: Time Series Forecasting in Ecology

When forecasting ecological time series:

1. **Model Selection**
   - Use `auto.arima()` for automatic model selection
   - Consider seasonal ARIMA for data with clear seasonality
   - Evaluate multiple models and compare AIC/BIC
   - Check residual diagnostics to validate model assumptions

2. **Forecast Interpretation**
   - Point forecasts are the expected values
   - Confidence intervals quantify uncertainty
   - Uncertainty increases with forecast horizon
   - Consider biological constraints (e.g., populations can't be negative)

3. **Ecological Considerations**
   - Short-term forecasts are more reliable than long-term
   - Regime shifts or environmental changes can invalidate models
   - Incorporate external covariates when available (e.g., climate indices)
   - Use ensemble forecasts for robust predictions

4. **Communication**
   - Always show uncertainty in forecasts
   - Explain assumptions and limitations
   - Relate forecasts to management thresholds
   - Update forecasts as new data become available
:::

### Ecological Example: Phenology Trends

```{r}
# Simulate phenological data (e.g., first flowering date)
set.seed(123)
years_pheno <- 1980:2023
n_years_pheno <- length(years_pheno)

# Earlier flowering over time due to climate change
flowering_doy <- 120 - 0.3 * (1:n_years_pheno) + rnorm(n_years_pheno, 0, 3)

pheno_data <- data.frame(
  year = years_pheno,
  flowering_day = round(flowering_doy)
)

# Visualize the trend
ggplot(pheno_data, aes(x = year, y = flowering_day)) +
  geom_point(color = "darkgreen", size = 2) +
  geom_smooth(method = "lm", color = "red", se = TRUE) +
  labs(
    title = "Phenological Shift: First Flowering Date",
    subtitle = "Earlier flowering over time suggests climate warming",
    x = "Year",
    y = "Day of Year (First Flowering)"
  ) +
  theme_minimal()

# Fit a linear model to quantify the trend
pheno_model <- lm(flowering_day ~ year, data = pheno_data)
summary(pheno_model)

# Calculate the rate of change
rate_per_decade <- coef(pheno_model)["year"] * 10
cat(sprintf("Flowering is occurring %.2f days earlier per decade\n", abs(rate_per_decade)))

# Test for autocorrelation in residuals
dwtest(pheno_model)

# If autocorrelation is present, use GLS
library(nlme)
gls_model <- gls(flowering_day ~ year,
                 correlation = corAR1(form = ~ year),
                 data = pheno_data)
summary(gls_model)
```

::: {.callout-important}
## Phenological Trends and Climate Change

Phenological shifts provide powerful evidence of climate change impacts:

1. **Trend Detection**
   - Linear regression can detect long-term trends
   - Rate of change quantifies the magnitude of shifts
   - Statistical significance indicates confidence in the trend

2. **Temporal Autocorrelation**
   - Phenological data often show autocorrelation
   - Durbin-Watson test detects autocorrelation in residuals
   - Generalized Least Squares (GLS) accounts for autocorrelation
   - Proper modeling prevents inflated Type I error rates

3. **Ecological Implications**
   - Earlier flowering can lead to phenological mismatches
   - Pollinators may not be active when plants flower
   - Affects reproductive success and population dynamics
   - Indicates ecosystem-wide climate change responses

4. **Conservation Applications**
   - Monitor phenological shifts as climate change indicators
   - Identify species vulnerable to phenological mismatches
   - Inform assisted migration decisions
   - Guide adaptive management strategies
:::

::: {.callout-note}
## Code Explanation

The phenology example demonstrates practical time series analysis:

1. **Trend Analysis**
   - Linear regression quantifies the rate of phenological shift
   - Confidence intervals indicate precision of the estimate
   - Visual inspection confirms the trend pattern

2. **Autocorrelation Testing**
   - Durbin-Watson test checks for temporal autocorrelation
   - Significant autocorrelation requires specialized models
   - GLS with AR(1) correlation structure accounts for autocorrelation

3. **Model Comparison**
   - Compare standard linear model with GLS
   - GLS provides more accurate standard errors
   - Prevents false positives in trend detection
:::

Time series analysis is essential for understanding temporal dynamics in ecological systems. By properly accounting for temporal structure, we can detect trends, make forecasts, and understand how ecosystems respond to environmental change.





```

### Logistic Regression with Tidymodels

```{r}
# Prepare binary classification data
penguins_binary <- penguins %>%
  mutate(is_adelie = factor(ifelse(species == "Adelie", "Adelie", "Other"),
                           levels = c("Other", "Adelie")))

# Split data
set.seed(2023)
binary_split <- initial_split(penguins_binary, prop = 0.75, strata = is_adelie)
binary_train <- training(binary_split)
binary_test <- testing(binary_split)

# Create recipe
logistic_recipe <- recipe(is_adelie ~ bill_length_mm + bill_depth_mm + flipper_length_mm,
                         data = binary_train) %>%
  step_normalize(all_numeric_predictors())

# Specify logistic regression model
logistic_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

# Create workflow
logistic_wf <- workflow() %>%
  add_recipe(logistic_recipe) %>%
  add_model(logistic_spec)

# Fit the model
logistic_fit <- logistic_wf %>%
  fit(data = binary_train)

# Get predictions with probabilities
logistic_pred <- logistic_fit %>%
  predict(binary_test, type = "prob") %>%
  bind_cols(logistic_fit %>% predict(binary_test)) %>%
  bind_cols(binary_test)

# Calculate multiple classification metrics
logistic_metrics <- logistic_pred %>%
  metrics(truth = is_adelie, estimate = .pred_class, .pred_Adelie)

knitr::kable(logistic_metrics,
             caption = "Logistic Regression Classification Metrics",
             digits = 3)

# Create confusion matrix
conf_mat(logistic_pred, truth = is_adelie, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  labs(title = "Confusion Matrix: Adelie Classification") +
  theme_minimal()

# ROC curve
logistic_pred %>%
  roc_curve(truth = is_adelie, .pred_Adelie) %>%
  autoplot() +
  labs(title = "ROC Curve: Tidymodels Logistic Regression") +
  theme_minimal()

# Calculate and display AUC
auc_value <- logistic_pred %>%
  roc_auc(truth = is_adelie, .pred_Adelie) %>%
  pull(.estimate)

cat("Area Under the Curve (AUC):", round(auc_value, 3), "\n")

# Visualize predicted probabilities
ggplot(logistic_pred, aes(x = bill_length_mm, y = bill_depth_mm,
                         color = .pred_Adelie, shape = is_adelie)) +
  geom_point(size = 3, alpha = 0.7) +
  scale_color_gradient2(low = "navy", mid = "white", high = "red",
                       midpoint = 0.5) +
  labs(
    title = "Predicted Probabilities: Adelie Classification",
    subtitle = paste0("AUC = ", round(auc_value, 3)),
    x = "Bill Length (mm)",
    y = "Bill Depth (mm)",
    color = "P(Adelie)",
    shape = "Actual Species"
  ) +
  theme_minimal()
```

::: {.callout-note}
## Tidymodels for Classification

The tidymodels classification workflow provides:

1. **Unified metrics**: Automatic calculation of accuracy, ROC AUC, and other metrics
2. **Probability predictions**: Easy access to class probabilities
3. **Visualization tools**: Built-in plotting for confusion matrices and ROC curves
4. **Consistent interface**: Same workflow structure as regression
:::

## Comparing Base R and Tidymodels Approaches

### When to Use Each Approach

**Base R (`lm()`, `glm()`) is ideal for:**
- Quick exploratory analyses
- Simple models with few predictors
- Teaching statistical concepts
- When you need specific model diagnostics
- Working with traditional statistical output

**Tidymodels is ideal for:**
- Production modeling pipelines
- Comparing multiple models
- Complex preprocessing requirements
- Cross-validation and resampling
- Hyperparameter tuning
- Consistent evaluation across models
- Reproducible research workflows

### Feature Comparison

| Feature | Base R | Tidymodels |
|---------|--------|------------|
| **Learning Curve** | Moderate | Steeper initially |
| **Consistency** | Varies by package | Unified interface |
| **Preprocessing** | Manual | Automated with recipes |
| **Cross-validation** | Manual setup | Built-in support |
| **Model Comparison** | Manual | Streamlined |
| **Hyperparameter Tuning** | Package-specific | Unified with tune |
| **Production Deployment** | More complex | Workflow objects |
| **Extensibility** | High | Very high |

::: {.callout-tip}
## PROFESSIONAL TIP: Best of Both Worlds

Many practitioners use both approaches:

1. **Exploration**: Use base R for quick model fitting and diagnostics
2. **Refinement**: Transition to tidymodels for rigorous evaluation
3. **Production**: Deploy tidymodels workflows for consistency
4. **Communication**: Use base R output for traditional audiences, tidymodels for modern reports
:::

## Exercises

### Base R Exercises

1. Fit a linear regression model predicting body mass from bill length and bill depth. Create a 3D visualization of this relationship using the `plotly` package.

2. Conduct multiple regression with interaction terms between predictors. How does adding interactions improve model performance?

3. Perform model selection using AIC to find the most parsimonious multiple regression model for the penguin data.

4. Create a multinomial logistic regression model to classify all three penguin species based on morphological traits.

5. Compare the performance of logistic regression with other classification methods (e.g., random forest, support vector machines) for species identification.

### Tidymodels Exercises

6. Create a tidymodels workflow for predicting flipper length from body mass and bill measurements. Use 5-fold cross-validation to evaluate performance.

7. Build a recipe that includes polynomial features (e.g., squared terms) for bill length and bill depth. Compare this model's performance to a linear model using cross-validation.

8. Use tidymodels to compare at least three different regression models (e.g., linear regression, random forest, support vector machine) for predicting body mass. Which performs best?

9. Create a classification workflow using tidymodels to predict all three penguin species. Calculate and compare precision, recall, and F1 scores for each species.

10. Implement a hyperparameter tuning workflow using `tune_grid()` to optimize the number of trees and minimum node size for a random forest model predicting body mass.

### Advanced Exercises

11. Split the penguin data by island, use one island as a test set, and evaluate how well models trained on other islands generalize. What does this tell you about geographical variation?

12. Create a nested cross-validation workflow: use an outer loop for model evaluation and an inner loop for hyperparameter tuning. Compare this to simple cross-validation.

13. Develop a complete modeling pipeline that includes:
    - Data exploration and visualization
    - Train/test split with stratification
    - Recipe with appropriate preprocessing
    - Multiple model specifications
    - Cross-validation comparison
    - Final model evaluation on test set
    - Interpretation of results

14. Use tidymodels to implement a regularized regression (ridge or lasso) and compare it to ordinary least squares. How does regularization affect the coefficients?

15. Create an ensemble model that combines predictions from linear regression, random forest, and gradient boosting models. Does the ensemble outperform individual models?